{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d00a813-0118-4ca7-b7a1-13e8110498b5",
   "metadata": {},
   "source": [
    "**Purpose of Day 6:**  \n",
    "Learn advanced optimization techniques to make `LLMs faster` and more `efficient` for real-world use (KV caching, flash attention, speculative decoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af06eba1-afdd-432a-9910-8507eaaef92a",
   "metadata": {},
   "source": [
    "**Purpose:** Demonstrate KV caching—store and reuse attention computations to speed up text generation.\n",
    "\n",
    "- **First call:** Computes full attention, caches key-value pairs.\n",
    "- **Second call:** Uses cached KV, only computes for new tokens → faster.\n",
    "\n",
    "Shows how production systems speed up consecutive generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded36f04-dee6-4aa9-8bc4-c590e52ec043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66adb662-6601-4872-9c85-d7d4f3c141fd",
   "metadata": {},
   "source": [
    "#### `1`  KV caching (speed boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec569d79-a43b-401d-8392-1f43582162ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HELLO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08dc7996-db21-4cb8-b0e6-8adfd6abfbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'distilgpt2'\n",
    "name = './gpt2-local'\n",
    "quantize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4eb5249-63cc-455e-bfe0-1d3a80551c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if quantize:\n",
    "    bnb = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        name, quantization_config=bnb, device_map='auto')\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        name, torch_dtype=torch.float16, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7421dcd7-16ba-4767-b808-adc8db24224c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# It sets the padding token to the end-of-sequence token so the model knows what to use for padding during generation. \n",
    "# Needed for proper batch handling and avoiding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c247df8b-b889-4aee-a922-026d2c5ba151",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0702fc9-2aec-4b72-a751-0ec3e7851ef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_with_cache(prompt, max_new_tokens=50, temperature=0.7, top_p=0.9):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        streamer = streamer,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True  # Internal caching only\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8da5353b-52c5-422b-85a3-366d551b98f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== First call ===\n",
      " is likely to be more complicated than we thought. AI is just the beginning of the development process.\n",
      "The future of AI is likely to be more complicated than we thought. AI is just the beginning of the development process.\n",
      "\n",
      "=== Continue same sequence ===\n",
      " and is a product of the human condition. We are in a state of profound distress. We are\n",
      " is and is a product of the human condition. We are in a state of profound distress. We are\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print(\"=== First call ===\")\n",
    "result1 = generate_with_cache(\"The future of AI\", max_new_tokens=20)\n",
    "print(result1)\n",
    "\n",
    "print(\"\\n=== Continue same sequence ===\")\n",
    "result2 = generate_with_cache(\" is\", max_new_tokens=20)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caedabd5-64ee-47b6-baae-e2d69170deeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3069d69e-dccd-4a04-ad8c-320379d576c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### `2`  Efficient Serving with vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d93a953-1aa3-4480-bd73-91a620cf098e",
   "metadata": {},
   "source": [
    "Those are vLLM's key speed features.\n",
    "\n",
    "- **PagedAttention:** Manages KV cache like OS virtual memory → less waste.\n",
    "- **Continuous batching:** Mixes requests dynamically → GPU always busy.\n",
    "- **Optimized CUDA kernels:** Custom GPU code → faster computations.\n",
    "\n",
    "Together they make vLLM the fastest for serving LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c06f7b-23c3-4264-9c21-c762d792a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vllm_serving_demo():\n",
    "\n",
    "    try:\n",
    "        from vllm import LLM, SamplingParams\n",
    "        \n",
    "        # Initialize LLM\n",
    "        llm = LLM(model=\"gpt2\", quantization=\"awq\", max_model_len=4096)\n",
    "        \n",
    "        # Sampling params\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        \n",
    "        # Batch prompts\n",
    "        prompts = [\n",
    "            \"Explain machine learning to a 10-year-old:\",\n",
    "            \"Write a Python function to calculate fibonacci:\",\n",
    "            \"The benefits of renewable energy include:\"\n",
    "        ]\n",
    "        \n",
    "        # Generate\n",
    "        outputs = llm.generate(prompts, sampling_params)\n",
    "        \n",
    "        # Print results\n",
    "        for output in outputs:\n",
    "            print(f\"Prompt: {output.prompt}\")\n",
    "            print(f\"Generated: {output.outputs[0].text}\\n\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"vLLM not installed. Install with: pip install vllm\")\n",
    "        print(\"This demonstrates the production serving approach\")\n",
    "\n",
    "# Run vLLM demo\n",
    "vllm_serving_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915d82b-587d-4e6b-8919-09128bcf3919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e20e221-93fa-4f2b-97bb-8ff7d7010d8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### `3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d87c66a6-ce1d-4ef7-b642-71680bf20965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HELLO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "079bf362-1dff-49d2-82a7-2a6ec3240e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "small = AutoModelForCausalLM.from_pretrained(\"./distilgpt2-local\")\n",
    "large = AutoModelForCausalLM.from_pretrained(\"./gpt2-local\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('./distilgpt2-local')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4b0d554-c2cf-4b3e-a42b-6e3aad997e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer =TextStreamer(tokenizer, skip_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4bc60b7-7467-44b4-bf3c-94983436f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speculative(prompt, max_tokens=50, lookahead=5, stream=True):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    generated = inputs.clone()\n",
    "\n",
    "    for s in range(max_tokens):\n",
    "        s_outputs = small.generate(\n",
    "            generated, max_new_tokens=lookahead, do_sample=True, temperature=0.7, pad_token_id=tokenizer.eos_token_id, \n",
    "            num_return_sequences=1)\n",
    "\n",
    "        tokens = s_outputs[0, generated.shape[1]:]\n",
    "\n",
    "        l_outputs = large(\n",
    "            s_outputs, output_hidden_states=False, output_attentions=False)\n",
    "\n",
    "        l_logits = l_outputs.logits\n",
    "        l_probs = torch.softmax(l_logits[:, -lookahead-1:-1], dim=-1)\n",
    "        \n",
    "        accept_token = []\n",
    "\n",
    "        for i, d in enumerate(tokens):\n",
    "            probs = l_probs[0, i, d]\n",
    "\n",
    "            if probs > 0.5:\n",
    "                accept_token.append(d.unsqueeze(0))\n",
    "            else:\n",
    "                l_token = torch.multinomial(l_probs[0, i], 1)\n",
    "                accept_token.append(l_token)\n",
    "                break\n",
    "\n",
    "        if not accept_token:\n",
    "            break\n",
    "            \n",
    "        new_tokens = torch.cat(accept_token)\n",
    "        generated = torch.cat([generated, new_tokens.unsqueeze(0)], dim=1)\n",
    "        \n",
    "        # Stream final accepted tokens only if requested\n",
    "        if stream:\n",
    "            streamer.put(new_tokens)\n",
    "        \n",
    "        if tokenizer.eos_token_id in new_tokens:\n",
    "            break\n",
    "    \n",
    "    if stream:\n",
    "        streamer.end()\n",
    "        \n",
    "    return tokenizer.decode(generated[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28548a38-a534-4057-b386-dc6e861edc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Artificial intelligence will transform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b12f560-aa48-4ea3-8d7f-c1c94b15d566",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " brains very rapidly, the report shows.\n",
      "\n",
      "This new approach to medicine, which looks at the human brain as a set of distinct parts that are really only \"together\n"
     ]
    }
   ],
   "source": [
    "result = speculative(prompt, max_tokens=30, lookahead=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bcd9d96-f34e-4c7f-b004-0c8d2c24c983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence will transform our brains very rapidly, the report shows.\n",
      "\n",
      "This new approach to medicine, which looks at the human brain as a set of distinct parts that are really only \"together\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614514cf-d46b-4013-b729-a4e2161748ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
