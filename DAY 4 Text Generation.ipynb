{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d3d708-7b3d-4e02-8a95-9319ba5011e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HELLO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86671214-1c6f-41ab-b1bb-19d0a9aeefd5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 0 : Load & LLMs (Get Model and Tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bdc007-f79e-420c-b5c7-25c405ff5756",
   "metadata": {},
   "source": [
    "**Temperature:** Controls randomness.  \n",
    "- Low (~0.1): Predictable, repetitive.  \n",
    "- High (~1.0): Creative, random.\n",
    "\n",
    "**Top-p (nucleus sampling):** Choose from top tokens whose probabilities sum to `p`.  \n",
    "- Keeps likely tokens, ignores long tail.\n",
    "\n",
    "**Beam search:** Keeps `k` best sequences each step.  \n",
    "- Better for tasks needing coherence (translation). Slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9972204c-ce58-499d-a0db-52da77287949",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     'distilgpt2',\n",
    "#     device_map = 'auto',\n",
    "#     torch_dtype = torch.float16,\n",
    "#     trust_remote_code = True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8601f1-dd75-4104-992c-fa4733d7eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(model='distilgpt2'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model) # Converts text ↔ tokens (vocabulary handling).\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained( # load model and how fast the model will go\n",
    "        model,\n",
    "        device_map = 'auto', # Auto-place on GPU/CPU\n",
    "        torch_dtype = torch.float16,  # Use half precision (saves memory)\n",
    "        trust_remote_code = True # Allow custom code from HF\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d36bd-da70-4afa-a0e6-be88be7ef3e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69d3e627-2b29-456f-9480-75fc99fa2057",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1 :  Advanced Generation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dd42078-a86f-4cd8-ab55-6ece9e08251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, **gene):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            **gene\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316afee-2155-40fb-8b9a-a8c376947140",
   "metadata": {},
   "source": [
    "This part:\n",
    "\n",
    "1. **`with torch.no_grad():`** – Disables gradient calculation (faster, less memory during inference).\n",
    "2. **`model.generate(**inputs, **gene)`** – Generates tokens using the model with your prompt and any extra parameters.\n",
    "3. **`tokenizer.decode(...)`** – Converts token IDs back to readable text.\n",
    "\n",
    "It's the core inference step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9eab82-dc88-4bcc-a386-5b729b256f8d",
   "metadata": {},
   "source": [
    "`**inputs`: Prompt tokenized as model input.\n",
    "\n",
    "`**gene`: Any extra generation parameters you pass (like `temperature`, `top_p`, `max_new_tokens`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bd374c2-b7ad-49fa-a44e-865c9ee10ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Greedy Decoding (basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "599efc62-ab68-420f-a043-c17b23596177",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00041941-4516-42db-93c1-84f6bb0ad4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "greedy_text = generate(\n",
    "    model, tokenizer, prompt, max_new_tokens=12, do_sample=False, num_beams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c44e2087-c2a6-4c3d-8220-21bfca8aeba1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello The U.S. Department of Justice has been investigating the\n"
     ]
    }
   ],
   "source": [
    "print(greedy_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf7b8b-8d0a-4fd7-8397-0ddeb5564cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "818e35ac-a6f8-42ac-8b78-a466b493d53e",
   "metadata": {},
   "source": [
    "**Greedy decoding:**  \n",
    "Model picks the highest-probability token at each step, 12 times total. Deterministic, no randomness.\n",
    "\n",
    "`do_sample=False`: Turns off random sampling → greedy search.  No randomness. Always picks the single most probable token.\n",
    "\n",
    "`num_beams=1`: Uses beam search width of 1 → basic greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c32bc3-8a94-4671-98db-deab2ff0d339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "554f3c71-507e-434d-8997-a7e486531ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 with Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81243b60-5cd9-4b0e-8b96-7df84bd409c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'what is love?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd0645a-deda-4fac-bd6c-8c600894a74c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "temp = generate(\n",
    "    model, tokenizer, prompt, max_new_tokens=15, do_sample = True, temperature=1.0, top_k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "455f19ec-9ec3-4ed9-b37d-6abfbfb44fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is love? Is love something, not love.\n",
      "\n",
      "The truth is love is not\n"
     ]
    }
   ],
   "source": [
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14fbd2d-2698-40ca-9fc0-d4bf272cac6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "567aa3c5-e944-407a-8c49-98a58499ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Nucleus Sampling (top-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fb0cac1-a8de-42da-82a2-6e25cb485808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "nucleus = generate(\n",
    "    model, tokenizer, prompt, max_new_tokens=10, do_sample=True,\n",
    "    top_p = 0.92, # Consider tokens comprising 92% of probability mass\n",
    "    temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae452c16-c505-4308-8c0d-8da10eb30e7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is love, friendship, friendship, and love, love,\n"
     ]
    }
   ],
   "source": [
    "print(nucleus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fbe3f6-5af6-4da4-8541-770ad8ca9bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8e2aedc-64df-4866-9351-143a6b4d9f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "655dd1b1-7618-47f6-b462-9401289f00a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is love'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f24fee86-f8b0-44d4-af0b-ce53600b1a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "text = generate(\n",
    "                model, tokenizer, prompt='Jesus Christ',\n",
    "                max_new_tokens=50,\n",
    "                do_sample=True,\n",
    "                temperature=1.0,\n",
    "                top_p=1.0,\n",
    "                # pad_token_id=tokenizer.eos_token_id\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84171b6f-147e-4b1e-a110-03cf54918a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jesus Christ for all the sins of men and of women, for the salvation of God, for the good of the universe, for all the good of humanity. ... That is true, and to the contrary, when God commands the world to go for Him,\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa0f53d-7ab7-4348-923d-676766e644e8",
   "metadata": {},
   "source": [
    "Default good values: do_sample=True, temperature=0.7, top_p=0.9. | ME : do_sample=True, temperature=1.0, top_p=1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa45ad-8d5e-47d3-9bb7-c4bf89098d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f70ae5a-b660-4ca6-a9a7-cad22891a6e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2 : Compare LLaMA vs Mistral Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3362f480-e31a-4a11-8531-a78b60a53590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MODEL: gpt2\n",
      "==================================================\n",
      "what is love? Is it not love?\n",
      "\n",
      "I am a woman. I am a male.\n",
      "\n",
      "I am a man. I am a woman.\n",
      "\n",
      "I am a man. I am a woman.\n",
      "\n",
      "I am a man. I am\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "MODEL: distilgpt2\n",
      "==================================================\n",
      "what is love?‷It would seem that it‷could have been‷‷t. it is true. it could mean that you‷you could probably be getting used to it, especially if you don‷ve thought you could love it\n",
      "-------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def compare_models(prompt, models_to_test):\n",
    "    \"\"\"Compare generation across different models\"\"\"\n",
    "    \n",
    "    for model_name in models_to_test:\n",
    "        try:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"MODEL: {model_name}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            model, tokenizer = load_llm(model_name)\n",
    "            \n",
    "            # Consistent generation parameters\n",
    "            text = generate(\n",
    "                model, tokenizer, prompt,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=True,\n",
    "                temperature=1.0,\n",
    "                top_p=1.0,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            print(text)\n",
    "            print(\"-------------------------------------------------------------------------------------------------------------------\")\n",
    "            # Clean up memory\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {model_name}: {e}\")\n",
    "\n",
    "# Test with smaller, accessible models\n",
    "test_models = [\n",
    "    \"gpt2\",                       # Base GPT-2\n",
    "    \"distilgpt2\"                  # Even smaller\n",
    "]\n",
    "\n",
    "compare_models(\"what is love?\", test_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ffeb91-ca0a-47fa-8cfa-114f22ba92a6",
   "metadata": {},
   "source": [
    "- `do_sample`: Enable random sampling (True) or greedy (False).\n",
    "- `temperature`: Controls randomness (0.1–1.0).\n",
    "- `top_p`: Nucleus sampling threshold (0.0–1.0).\n",
    "- `pad_token_id`: Token ID for padding (often EOS token)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0547f30f-3e63-4533-8ea6-6a672c65b84c",
   "metadata": {},
   "source": [
    "- `temperature` scales all logits (sharpens/flattens distribution).\n",
    "- `top_p` cuts off low-probability tail (removes unlikely tokens).\n",
    "- `do_sample` enables/disables sampling entirely.\n",
    "\n",
    "They work together for balanced randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359de87f-560e-418f-9c21-0c9991471f57",
   "metadata": {},
   "source": [
    "Default good values: do_sample=True, temperature=0.7, top_p=0.9. | ME : do_sample=True, temperature=1.0, top_p=1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b039c24-38ea-4a14-a1c9-9449ea38121a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f7072c7-6140-4520-8157-1d1176ab6786",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3 . Advanced: Streaming Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f610d74d-a971-4d5a-a009-1bb8a6b70292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_generate(model, tokenizer, prompt, max_tokens=50):\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    generated = inputs.input_ids.clone()\n",
    "\n",
    "    print('streaming:', end=\" \")\n",
    "\n",
    "    for i in range(max_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(generated)\n",
    "            next_token = outputs.logits[:, -1, :]\n",
    "\n",
    "            next_tokens = next_token / 0.7\n",
    "            the_tokens = torch.multinomial(\n",
    "                torch.softmax(next_tokens, dim=-1), num_samples=1)\n",
    "\n",
    "            generated = torch.cat([generated, the_tokens], dim=1)\n",
    "\n",
    "            new_text = tokenizer.decode(the_tokens[0], skip_special_tokens = True)\n",
    "            print(new_text, end='', flush=True)\n",
    "\n",
    "            if the_tokens.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0ce046b-9ba5-4b6e-be88-370a8da72546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "streaming:  to be lived there and we have to remain here.�� The meaning of life is to be lived in a world of all people with dignity and dignity and respect. Life is to be lived in a world of all people with dignity and respect.\n"
     ]
    }
   ],
   "source": [
    "# Test streaming\n",
    "stream_generate(model, tokenizer, \"The meaning of life is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a827bb-a1e2-408e-998e-c001f8a28299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "811b1704-d7ac-4a40-85dc-a465f92f843e",
   "metadata": {},
   "source": [
    "- `torch.multinomial` + softmax → equivalent to `do_sample=True`.\n",
    "- `next_token / 0.7` → **temperature** (0.7 here).\n",
    "- No `top_p` filtering in this code.\n",
    "\n",
    "Hardcoded to temperature=0.7 with sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a940bdf-ad8c-4e9f-95a4-79bd9debea22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b2808ba-9fbe-48b7-9334-50415bd9deed",
   "metadata": {},
   "source": [
    "#### $ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "357a7ef9-5115-4750-8553-fad1a17a77ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HELLO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f120b76-82a8-46e6-9106-72b0ca329e60",
   "metadata": {},
   "source": [
    "- `AutoTokenizer`: Loads correct tokenizer for model.\n",
    "- `AutoModelForCausalLM`: Loads causal language model (for text generation).\n",
    "- `GenerationConfig`: Stores default generation parameters (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f99ba58-396b-4d2d-af49-d4dd34c87b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "name  = 'distilgpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9209d7e-825c-40d6-bec1-bb0ef13614d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6e968b2-8501-40ef-b8ce-ffadebcdfa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    name,\n",
    "    device_map = 'auto', # auto-place on gpu/cpu\n",
    "    torch_dtype = torch.float16, # save memory\n",
    "    trust_remote_code = True # Allows execution of custom code from Hugging Face (for newer/unconventional models). Safer to keep `False` unless needed.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c395d03-86a1-4d44-9fab-e54b31994494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, **gene):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gene)\n",
    "        \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd8f0f9-1787-44a6-92d5-cc52babc9ab6",
   "metadata": {},
   "source": [
    "- `return_tensors='pt'`: Returns PyTorch tensors.\n",
    "- `.to(model.device)`: Moves tensors to same device as model (GPU/CPU).\n",
    "\n",
    "`**inputs`: Prompt tokenized as model input.\n",
    "\n",
    "`**gene`: Any extra generation parameters you pass (like `temperature`, `top_p`, `max_new_tokens`).\n",
    "\n",
    "- `torch.no_grad()`: Disables gradient tracking (faster inference, less memory).\n",
    "- `model.generate()`: Runs the model to generate text using the inputs and generation parameters.\n",
    "\n",
    "- `outputs[0]`: First sequence in batch.\n",
    "- `skip_special_tokens=True`: Removes special tokens like `<s>`, `</s>`.\n",
    "- Returns clean generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91276527-7795-416b-83fe-86c52c8ad3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'salvation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c264425c-4cdb-4a4f-8068-17d5eb21c81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "text = generate(\n",
    "    model, tokenizer, prompt,\n",
    "    max_new_tokens=50,\n",
    "    do_sample = True,\n",
    "    temperature = 1.0,\n",
    "    top_p = 1.0,\n",
    "    # pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "991b40c0-82cd-49e2-ac76-86b9ed46dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salvation of their heart.\n",
      "For a very deep and deeply spiritual reason, we must believe in prayer and the sacrament of mercy. You are a man who has experienced this suffering.\n",
      "In our heart, we are blessed to receive the sacrament of mercy and\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cca67763-651a-450f-b007-166cbe4406e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8806e84d-3899-492b-94b6-5ff24194ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(model, tokenizer, prompt, max_tokens=50):\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    generated = inputs.input_ids.clone()\n",
    "    \n",
    "    # Copies the initial token IDs from the prompt.\n",
    "    # Creates a tensor that will be extended with new tokens during generation.\n",
    "\n",
    "    print(\"streaming: \", end=\" \")\n",
    "    for i in range(max_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(generated)\n",
    "            tokens = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # Gets logits for the **second token position** (`[:, 1, :]`).\n",
    "            # - `outputs.logits`: Shape `[batch_size, sequence_length, vocab_size]`.\n",
    "            # - `[:, 1, :]`: Index 1 → second token in sequence.\n",
    "\n",
    "            tokens = tokens / 0.7\n",
    "            dtokens = torch.multinomial(\n",
    "                torch.softmax(tokens, dim=-1), num_samples=1)\n",
    "            \n",
    "            # 1. `tokens / 0.7`: Applies **temperature** scaling (0.7 = moderate randomness).\n",
    "            # 2. `torch.softmax(tokens, dim=-1)`: Converts logits to probabilities.\n",
    "            # 3. `torch.multinomial(..., num_samples=1)`: Samples 1 token from the probability distribution (random sampling).\n",
    "            # Equivalent to `do_sample=True, temperature=0.7`.\n",
    "            # for no sample: \n",
    "                # next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "            generated = torch.cat([generated, dtokens], dim=1)\n",
    "            # Appends the newly generated token (dtokens) to the existing sequence (generated) along the sequence dimension (dim=1).\n",
    "            # Because autoregressive generation builds the output sequentially.\n",
    "            # Each step adds one new token to the sequence, then feeds the extended sequence back into the model to predict the next token.\n",
    "\n",
    "            new_text = tokenizer.decode(dtokens[0], skip_special_tokens = True)\n",
    "            print(new_text, end=\"\", flush=True)\n",
    "           \n",
    "            # Decodes the single new token into text and prints it immediately.\n",
    "            # - `end=\" \"`: Keeps printing on same line with a space.\n",
    "            # - `flush=True`: Forces immediate display (for real-time streaming).\n",
    "\n",
    "            if dtokens.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d00aa1f-f130-4f25-ae5f-e20e5d68ad12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "streaming:   of the New Testament, Acts, and Secret Acts, the City of New York (New York: George Washington University Press), and the City of New York (New York: George Washington University Press), and and the City of New York (New York\n"
     ]
    }
   ],
   "source": [
    "sam = stream(model, tokenizer, 'salvation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7fa6663-afb6-4833-837f-27ca46549098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dbae3f8-b2e1-4e61-9193-740fbf494f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86d62851-96ef-48d6-b91e-fb580ee0a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f2ef125-198b-480e-83fa-fb1fb203e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, **gene):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, streamer=streamer, **gene)\n",
    "        \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea8f19ae-5213-499b-ade6-82983da83b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Python programming'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ba5bafb-b23e-4b6c-97ba-7cdfeb1e854a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " language can be useful for many tasks in the programming language. A full-blown language can provide a great experience for working with specific language-based programming languages. Some popular languages include C++, JRuby, Java or Java to understand the most common\n"
     ]
    }
   ],
   "source": [
    "text = generate(\n",
    "    model, tokenizer, prompt,\n",
    "    max_new_tokens=50,\n",
    "    do_sample = True,\n",
    "    temperature = 1.0,\n",
    "    top_p = 1.0,\n",
    "    # pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d71d61f-e75b-47d5-8f76-190c0d1f4798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c32d3-ec81-4053-b8f7-dc5acb3dfd87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce51287-4e5e-4faf-98f4-5dd5c9c5be18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
