{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e37cce-c0cf-4361-b21e-5a48cbd170ad",
   "metadata": {},
   "source": [
    "**Day 9 topics:**\n",
    "1. **LoRA (Low-Rank Adaptation)** – Efficient fine-tuning method\n",
    "2. **QLoRA** – LoRA with 4-bit quantization (even more efficient)\n",
    "3. **Fine-tuning pipeline** – Complete training setup\n",
    "4. **Parameter efficiency** – Training 0.01%-0.1% of model weights\n",
    "\n",
    "**Goal:** Teach **how to customize LLMs efficiently** for specialized tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bcdaa7-b012-454d-bdff-80234907504f",
   "metadata": {},
   "source": [
    "**1. LoRA** → Train only **tiny matrices** added to model, not all weights. Saves memory/time.\n",
    "\n",
    "**2. QLoRA** → LoRA but with **model weights in 4-bit** (not 16-bit). Even more memory saved.\n",
    "\n",
    "**3. Fine-tuning pipeline** → **Code structure** to: load data, train model, evaluate, save.\n",
    "\n",
    "**4. Parameter efficiency** → Train **~100,000 params** instead of **7,000,000,000**. Makes fine-tuning possible on consumer hardware.\n",
    "\n",
    "**Simple analogy:**  \n",
    "Instead of rebuilding a car engine (full fine-tuning), just **add a small chip** (LoRA) that makes it drive better for your specific roads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab6386-1c25-433b-85c5-dde6ae36f7de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1: LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d84018a-2993-4cad-b1aa-5357f158c98a",
   "metadata": {},
   "source": [
    "**LoRA = Small \"adapter\" matrices** added to a frozen model.\n",
    "\n",
    "**How it works:**\n",
    "1. **Freeze** the original LLM (no training).\n",
    "2. **Add** two tiny matrices (`A` and `B`) to each layer.\n",
    "3. **Train only** these tiny matrices.\n",
    "4. During inference: `Output = Original + (A × B × input)`.\n",
    "\n",
    "**Example:**  \n",
    "Instead of training **1 billion parameters**, train only **0.1 million** (the `A` and `B` matrices).\n",
    "\n",
    "**Result:** Customized model with **1/1000th the training cost**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fec1f6-dc5c-4ba6-951c-8f3ea2887596",
   "metadata": {},
   "source": [
    "**LoRA lets you customize a giant LLM on a normal computer.**\n",
    "\n",
    "**Good it does:**\n",
    "1. **Makes LLMs fit your task** – Train it to excel at your specific format (like your `TOOL:` prompt).\n",
    "2. **Saves huge memory** – Train 0.1% of parameters instead of 100%.\n",
    "3. **Fast training** – Hours, not weeks.\n",
    "4. **Keep base model** – One base model, many LoRA adapters for different tasks.\n",
    "\n",
    "**For you:** Could train Llama to **perfectly follow** your agent's `TOOL:` format, making it more reliable than prompting alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f70915-c627-4d24-8c91-5959429c5385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how:\n",
    "# But you could change it to train on your agent's tool-use examples to make it better at following TOOL: format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d8ae3df-cbff-4ad1-b608-feed73ccd2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HELLO!\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # Import PyTorch neural network modules\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db40af3-0a87-497b-beec-0f9b4f61b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lora(base_layer, rank=8, alpha=16):\n",
    "    for p in base_layer.parameters():\n",
    "        p.requires_grad = False\n",
    "    lora_A = nn.Linear(base_layer.in_features, rank, bias=False)\n",
    "    lora_B = nn.Linear(rank, base_layer.out_features, bias=False)\n",
    "    nn.init.kaiming_uniform_(lora_A.weight, a=5**0.5)\n",
    "    nn.init.zeros_(lora_B.weight)\n",
    "    return {\n",
    "        'base': base_layer,\n",
    "        'lora_A': lora_A,\n",
    "        'lora_B': lora_B,\n",
    "        'scale': alpha / rank\n",
    "    }\n",
    "\n",
    "def forward_lora(lora_dict, x):\n",
    "    base_out = lora_dict['base'](x)\n",
    "    lora_out = lora_dict['lora_B'](lora_dict['lora_A'](x))\n",
    "    return base_out + lora_dict['scale'] * lora_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "265df41b-b27a-4cb7-bf24-322a326a72dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-local\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-local\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc22382-7068-4a43-8221-78b0b8213ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA applied to GPT-2. Ready for IMDB fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA to first layer (demo)\n",
    "if hasattr(model.transformer.h[0].mlp, 'fc1'):\n",
    "    lora = init_lora(model.transformer.h[0].mlp.fc1, rank=4)\n",
    "    model.transformer.h[0].mlp.fc1.forward = lambda x: forward_lora(lora, x)\n",
    "\n",
    "print(\"LoRA applied to GPT-2. Ready for IMDB fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "382152a3-0c45-499a-a318-5befd505c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add IMDB dataset and training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99205f73-ac91-491d-8eaa-cf2cae9b86df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HELLO!\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HELLO!\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████████████████████████████████████| 25000/25000 [00:00<00:00, 29779.81 examples/s]\n",
      "Generating test split: 100%|██████████████████████████████████████████| 25000/25000 [00:00<00:00, 118035.16 examples/s]\n",
      "Generating unsupervised split: 100%|███████████████████████████████████| 50000/50000 [00:01<00:00, 44106.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load IMDB\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_data = dataset[\"train\"].select(range(1000))  # First 1000 reviews\n",
    "test_data = dataset[\"test\"].select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c37923aa-1aa2-4576-a8e1-1e4efe8c883f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 275.36 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 234.42 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 250, Test batches: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "train_data = train_data.map(tokenize, batched=True)\n",
    "test_data = test_data.map(tokenize, batched=True)\n",
    "train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "test_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=4)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de72377f-bf44-4123-9af5-a9b4ab0d0351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42595c05-7898-46f7-8879-f04384552f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer only for LoRA parameters\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.AdamW(trainable_params, lr=1e-3)\n",
    "\n",
    "model.train()\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    inputs = {k: v.to(model.device) for k, v in batch.items()}\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch_idx % 50 == 0:\n",
    "        print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606f2b4-a8d0-4e6b-b591-c9177464da91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "843efa19-274e-4954-ae9c-3d5602f27cb8",
   "metadata": {},
   "source": [
    "**No.** Fine-tuning and using documents as context are **different things**.\n",
    "\n",
    "| | **Fine-Tuning** | **Documents as Context (RAG)** |\n",
    "| :--- | :--- | :--- |\n",
    "| **What** | **Retrain model weights** on your data | **Add documents to prompt** at query time |\n",
    "| **Changes model?** | Yes – permanently | No – uses existing model |\n",
    "| **Best for** | Teaching **new patterns** (like `TOOL:` format) | Adding **knowledge/facts** (like your docs) |\n",
    "| **Example** | Make GPT-2 output `TOOL:` reliably | Give GPT-2 your manual to answer questions about it |\n",
    "\n",
    "**For your agent:**\n",
    "- **Fine-tune** → Make it better at `TOOL:` format.\n",
    "- **Documents as context** → Let it answer questions about your documents (but not use tools).\n",
    "\n",
    "They're **separate techniques** you can combine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a416e7f-dd23-47a8-af28-bd8636b0cbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1615f5b8-1b27-4c76-99db-9dd9c44ea2a6",
   "metadata": {},
   "source": [
    "**No.**\n",
    "\n",
    "Fine-tuning **adapts** the model, **doesn't replace all knowledge**.\n",
    "\n",
    "**What changes:**\n",
    "- **Adds new patterns** (like your `TOOL:` format)\n",
    "- **Strengthens existing** relevant knowledge (movie reviews for IMDB)\n",
    "- **Weakens unrelated** knowledge (catastrophic forgetting)\n",
    "\n",
    "**Result:** Model **knows both** old and new, but **better at new task**. Like a doctor who **specializes in cardiology** but still knows general medicine—just better at heart issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3c09a-b79b-46d2-9295-1ca7353e2fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e63dd48-ca9d-435e-87a7-0e5635563778",
   "metadata": {},
   "source": [
    "**No.** Using your own documents **doesn't change the model's knowledge at all**.\n",
    "\n",
    "**How it works:**\n",
    "1. You **add your documents** to the prompt.\n",
    "2. The model **reads them** each time.\n",
    "3. It **answers based on** those documents + its existing knowledge.\n",
    "4. **Model weights stay unchanged.**\n",
    "\n",
    "**It's like giving a book to a student for an open-book test.** The student (model) reads it to answer, but doesn't memorize it permanently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38020790-6899-4022-b00b-5a5b53edfbe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0075cdf1-2c74-4b47-9141-51d1aafc16fc",
   "metadata": {},
   "source": [
    "**Pretraining.**\n",
    "\n",
    "**Pretraining** is the initial training on massive data (text, images, code) that **creates all the model's knowledge from scratch**.\n",
    "\n",
    "**What it does:**\n",
    "- Takes **random weights**\n",
    "- Trains on **terabytes of data** (internet, books, etc.)\n",
    "- Builds **all knowledge** the model will ever have\n",
    "\n",
    "**Fine-tuning** and **documents** just **adjust** or **use** this pretrained knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbe5d38-f095-4ac0-9cb5-c9070cdd2cba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40887670-134d-4aed-a24d-da53e1d46289",
   "metadata": {},
   "source": [
    "**No.** You **cannot** use an existing model for pretraining.\n",
    "\n",
    "**Pretraining** = **creating** a model from random weights + massive data.  \n",
    "**Existing model** = **already pretrained**—the pretraining is **done**.\n",
    "\n",
    "You can only:\n",
    "1. **Fine-tune** it (adjust existing knowledge)\n",
    "2. Use **prompting/RAG** (use existing knowledge)\n",
    "\n",
    "To \"pretrain\" from scratch, you need **billions of examples** and **massive compute** (months on thousands of GPUs). Not feasible for individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b5139c-4730-4e83-9d33-1de8cd60408c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6a5ea1-6d42-44e2-8096-691440376c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
