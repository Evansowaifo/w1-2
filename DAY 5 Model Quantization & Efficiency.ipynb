{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ecf5d3-9ea8-4b8f-8d88-59f6fccb864b",
   "metadata": {},
   "source": [
    "**Day 5 Purpose:**  \n",
    "Learn model quantization (FP16, 8-bit, 4-bit) to run LLMs with less memory. Compare speed/accuracy trade-offs. Apply to load bigger models on limited hardware.\n",
    "\n",
    "`use less memory`\n",
    "\n",
    "Learn to shrink LLMs (quantization) to run them with less memory, while measuring speed/quality trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aadb415-b05a-4456-bf1a-9416d6c1cdba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a60f5f4-8676-44dc-9ac3-c3f3c7e1cc93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 0 : checking speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9305785b-6775-4dea-a6d3-4680e8d77d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_quant(model='distilgpt2'):\n",
    "\n",
    "    # fp16\n",
    "    fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "        model, torch_dtype=torch.float16, device_map='auto')\n",
    "\n",
    "    # 8 bit\n",
    "    m_8bits = AutoModelForCausalLM.from_pretrained(\n",
    "        model, load_in_8bit=True, device_map='auto')\n",
    "\n",
    "    # 4 bit + BitsAndBytesConfig\n",
    "    bnb = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype=torch.float16)\n",
    "    \n",
    "    m_4bits = AutoModelForCausalLM.from_pretrained(\n",
    "        model, quantization_config=bnb, device_map='auto')\n",
    "    \n",
    "    return fp16, m_8bits, m_4bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183fda3a-b792-47c8-873d-7caa269c7d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Loading quantized models...\")\n",
    "f16, m_8bit, m_4bit = load_quant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c056dde-85c3-449c-91c6-ba15356fe0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9350a622-2036-428a-bbc8-acd98b6bd15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e4871d-533c-4283-9672-e501369451db",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(today)\n",
    "\n",
    "prompt = \"Hello world\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(f16.device)\n",
    "outputs = f16.generate(**inputs, max_new_tokens=30, do_sample=True, temperature=1.0, top_p=1.0)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "today = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c667db2b-13e1-438b-91ec-f5689a4d5647",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(today)\n",
    "\n",
    "prompt = \"Hello world\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(m_8bit.device)\n",
    "outputs = m_8bit.generate(**inputs, max_new_tokens=30, do_sample=True, temperature=1.0, top_p=1.0)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "today = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fb99e4-bcc1-4ec5-bbb3-747852e1ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(today)\n",
    "\n",
    "prompt = \"Hello world\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(m_4bit.device)\n",
    "outputs = m_4bit.generate(**inputs, max_new_tokens=30, do_sample=True, temperature=1.0, top_p=1.0)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "today = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30278615-7bec-424a-9407-ae78e17ef749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b203ae4-a044-4480-bd94-df42b2a8dff8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1 : Cache Memory & Speed Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b089bcb-e66f-47c7-a7aa-f29241bde7d4",
   "metadata": {},
   "source": [
    "**Purpose:** Measure and compare memory usage and generation speed across different quantization levels (FP16, 8-bit, 4-bit) of the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113cf84-fa87-4e3e-bd09-5e8119b6d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca0260-3b90-4b47-b57f-078141246dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(model, tokenizer, prompt, model_name):\n",
    "\n",
    "    # cache memory stuffs\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        memory_before = torch.cuda.memory_allocated() / 1024**3\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "    # time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, max_new_tokens=100, do_sample=True, temperature=0.7)\n",
    "        \n",
    "    end_time = time.time()\n",
    "\n",
    "        # Memory after\n",
    "    if torch.cuda.is_available():\n",
    "        memory_after = torch.cuda.memory_allocated() / 1024**3\n",
    "        memory_used = memory_after - memory_before\n",
    "    else:\n",
    "        memory_used = 0\n",
    "    \n",
    "    generation_time = end_time - start_time\n",
    "    tokens_generated = outputs.shape[1] - inputs.input_ids.shape[1]\n",
    "    tokens_per_second = tokens_generated / generation_time\n",
    "\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    print(f\"Memory used: {memory_used:.2f} GB\")\n",
    "    print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "    print(f\"Tokens/second: {tokens_per_second:.2f}\")\n",
    "    print(f\"Text: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")\n",
    "    \n",
    "    return memory_used, tokens_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea478d47-33b2-44ac-b547-e29c979d9fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The future of renewable energy will be\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e930e4-e340-49cd-a76a-5e6745f1e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"FP16\": f16,\n",
    "    \"8-bit\": m_8bit, \n",
    "    \"4-bit\": m_4bit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd4c491-3b82-46fa-8a0c-94ccff1a0347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for name, model in models.items():\n",
    "    memory, speed = benchmark(model, tokenizer, prompt, name)\n",
    "    results[name] = {\"memory_gb\": memory, \"tokens_per_sec\": speed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cbfdee-d268-4468-9f85-6b05f72c4996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7873b2af-28f1-49df-a366-94a210900bd9",
   "metadata": {},
   "source": [
    "8-bit fastest (5.03 tokens/sec).\n",
    "\n",
    "FP16 slower (2.46 tokens/sec).\n",
    "\n",
    "4-bit slowest (2.34 tokens/sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53885268-1380-4804-96a2-91ecb9528adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "# # 8-bit\n",
    "# bnb_8bit = BitsAndBytesConfig(load_in_8bit=True)\n",
    "# model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "#     model, quantization_config=bnb_8bit, device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# # 4-bit\n",
    "# bnb_4bit = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16\n",
    "# )\n",
    "# model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "#     model, quantization_config=bnb_4bit, device_map=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ecaa9-dfbe-4f2c-9f8e-b2a0123cf865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62db909a-cb1e-4b21-b7f3-73536003621a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e0a3c77-e98e-4e28-8041-f16961e3c3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HELLO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a18ca853-5456-40b6-a3ff-a4a84f95e6d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = \"distilgpt2\"\n",
    "quantize = True\n",
    "\n",
    "if quantize:\n",
    "    bnb = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        name, quantization_config = bnb, device_map = 'auto')\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        name, torch_dtype = torch.float16, device_map = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "293c511f-b82e-42dc-8366-8705cc71b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b22a83cf-aa70-462d-a6a8-73f1e6fd7bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_new_tokens=50, do_sample=True, temperature=1.0, top_p=1.0):\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be6f4540-3af8-4604-9869-7a66d7369204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lawsuit against Thessaloniki and the New Testament. The New Testament uses a metaphor for the death and resurrection of Jesus Christ – and the resurrection of Jesus Christ. The New Testament uses an allegory of the death and resurrection of Jesus Christ – and the\n"
     ]
    }
   ],
   "source": [
    "prompt = 'lawsuit'\n",
    "print(generate(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3666442-9341-46f2-ad44-41d7d30b180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"What is AI?\", \"Diligents.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2415315-ef9d-49a5-8386-0700ca6f9ef5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is AI?\n",
      "\n",
      "\n",
      "How does that sound?\n",
      "It sounds like I'm on a mission to get to my house when I need a little help!\n",
      "Why not do you know I've been doing this for a while that I'm working on?\n",
      "\n",
      "\n",
      "Diligents. These compounds, of course, are so abundant, that the formation of different substances was not thought to have been sufficient. They were then not found to have a fundamental or fundamental function in a brain; it was an organoid organoid. These compounds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in prompts:\n",
    "    text = generate(model, tokenizer, i)\n",
    "    print(text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf28050-2eb7-42e0-b348-48f30b82bfb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce697e0-27ee-454a-847d-b7bfad386d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
