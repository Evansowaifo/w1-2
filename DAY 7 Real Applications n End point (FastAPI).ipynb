{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab5267ec-c4ea-475d-897e-a3deb1e4289f",
   "metadata": {},
   "source": [
    "**Purpose of Day 7:** Build a **production-ready LLM API** with FastAPI, including streaming, caching, monitoring, and a client.\n",
    "\n",
    "It's about turning your model into a **real web service** that others can use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19c2e69-2d08-4840-8602-110b44043a2b",
   "metadata": {},
   "source": [
    "* Model Serving: vLLM, TensorRT-LLM, Triton\n",
    "* API Layer: FastAPI, Flask, gRPC\n",
    "* Caching: Redis for prompt/response caching\n",
    "* Monitoring: Logging, metrics, tracing\n",
    "* Safety: Content filtering, rate limiting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee49d08-b789-46d8-a4c4-6180904f6b1b",
   "metadata": {},
   "source": [
    "- **Model Serving:** Engine to run models efficiently.\n",
    "- **API Layer:** Expose model as web service.\n",
    "- **Caching:** Store responses to avoid recomputation.\n",
    "- **Monitoring:** Track performance and errors.\n",
    "- **Safety:** Protect against abuse/bad content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be3cd27-2315-4fc9-b3d5-b1867dcdb397",
   "metadata": {},
   "source": [
    "**Purpose:** Store generated responses so identical prompts don't need re-generation.\n",
    "\n",
    "**Usefulness:**\n",
    "1. **Speed:** Cache hits are instant (ms) vs generation (seconds)\n",
    "2. **Cost:** Reduces compute/API costs\n",
    "3. **Scalability:** Handles more users with same resources\n",
    "4. **Consistency:** Same prompt → same response\n",
    "\n",
    "**Example:** If 100 users ask \"What is AI?\", generate once, cache it, serve from cache 99 times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0206835c-875c-4ce0-b3e1-68c72f1e01d6",
   "metadata": {},
   "source": [
    "Yes:\n",
    "\n",
    "1. **Larger batch size:** Generate multiple tokens at once\n",
    "2. **Speculative decoding:** Small model drafts, large model verifies\n",
    "3. **Optimized kernels:** Use FlashAttention, custom CUDA kernels\n",
    "4. **KV caching:** Already using (`use_cache=True`)\n",
    "5. **Quantization:** 4-bit/8-bit models (already using)\n",
    "6. **Hardware:** GPU instead of CPU\n",
    "\n",
    "But token-by-token streaming will always feel slower than batch display. The **total time** is same, just perception differs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7cb1fb-78f8-430d-b128-487f618f4cac",
   "metadata": {},
   "source": [
    "**Purpose:** Create a **client** to call your FastAPI LLM server from Python.\n",
    "\n",
    "**Usefulness:**\n",
    "1. **Programmatic access:** Use your API in other Python scripts\n",
    "2. **Testing:** Verify server works correctly\n",
    "3. **Integration:** Connect to your LLM from other applications\n",
    "4. **Streaming support:** Handle both regular and streaming responses\n",
    "\n",
    "**Why:** Your API isn't useful without clients. This provides a ready-to-use Python client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4e7c7-548e-4397-a599-29d1d5cc78ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9a93629-0097-4f91-9ed5-2bbe96327b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HELLO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import asyncio\n",
    "import uuid\n",
    "import time\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c083cdb-7ccf-4323-92f9-2157795237f6",
   "metadata": {},
   "source": [
    "These are **imports** - bringing in all the tools needed:\n",
    "\n",
    "1. **FastAPI, HTTPException** - for building the web server\n",
    "2. **BaseModel** - for defining data structures (request/response format)\n",
    "3. **torch** - PyTorch, the AI framework\n",
    "4. **AutoModelForCausalLM, AutoTokenizer** - Hugging Face tools to load AI models\n",
    "5. **asyncio** - for async/await (handling multiple requests)\n",
    "6. **uuid** - to generate unique IDs for each request\n",
    "7. **time** - to measure how long generation takes\n",
    "8. **List, Optional** - type hints for better code clarity\n",
    "\n",
    "Without these imports, the code won't run because it doesn't know where these tools come from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c701d6-4fb8-4c7d-b491-957defd4e9b8",
   "metadata": {},
   "source": [
    "Yes, but only **basics**.\n",
    "\n",
    "For FastAPI, you just need to know:\n",
    "- `@app.get(\"/path\")` → handles GET requests\n",
    "- `@app.post(\"/path\")` → handles POST requests\n",
    "- `@app.on_event(\"startup\")` → runs code when server starts\n",
    "\n",
    "That's it for now. Learn more as you need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57538570-7ece-4d79-9ead-4e16ce073baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_tokens: int = 100\n",
    "    temperature: float = 0.7\n",
    "    top_p: float= 0.9\n",
    "    stream: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbdaa304-9fec-4315-9d51-4c7d7db10206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationResponse(BaseModel):\n",
    "    generated_text: str\n",
    "    request_id: str\n",
    "    token_generated: str\n",
    "    inference: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00543d90-0198-4b9c-96e6-437f51ea1400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05c62f51-fb8d-44f4-aa3c-5d95ede8226d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HELLO\\AppData\\Local\\Temp\\ipykernel_11244\\1643280293.py:25: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")  # Runs when server starts\n"
     ]
    }
   ],
   "source": [
    "# **Line-by-line explanation:**\n",
    "\n",
    "# ```python\n",
    "# Request/Response models\n",
    "class GenerationRequest(BaseModel):\n",
    "    prompt: str                    # User's input text (required)\n",
    "    max_tokens: int = 100          # How many new tokens to generate (default 100)\n",
    "    temperature: float = 0.7       # Randomness control (default 0.7)\n",
    "    top_p: float = 0.9             # Token selection limit (default 0.9)\n",
    "    stream: bool = False           # Whether to stream tokens (default False)\n",
    "\n",
    "class GenerationResponse(BaseModel):\n",
    "    generated_text: str            # The AI-generated text\n",
    "    request_id: str                # Unique ID for this request\n",
    "    tokens_generated: int          # How many tokens were made\n",
    "    inference_time: float          # How long generation took (seconds)\n",
    "\n",
    "# Initialize FastAPI app - creates the web server\n",
    "app = FastAPI(title=\"LLM Inference API\", version=\"1.0\")\n",
    "\n",
    "# Global model and tokenizer - will be loaded once and reused\n",
    "MODEL = None\n",
    "TOKENIZER = None\n",
    "\n",
    "@app.on_event(\"startup\")  # Runs when server starts\n",
    "async def load_model():\n",
    "    \"\"\"Load model on startup\"\"\"\n",
    "    global MODEL, TOKENIZER        # Use the global variables\n",
    "    print(\"Loading model...\")\n",
    "    \n",
    "    # Load the tokenizer (text ↔ tokens converter)\n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    TOKENIZER.pad_token = TOKENIZER.eos_token  # Set padding token\n",
    "    \n",
    "    # Load the actual AI model\n",
    "    MODEL = AutoModelForCausalLM.from_pretrained(\n",
    "        \"gpt2\",\n",
    "        torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "        device_map=\"auto\"           # Put on GPU if available\n",
    "    )\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "@app.post(\"/generate\", response_model=GenerationResponse)  # POST endpoint at /generate\n",
    "async def generate_text(request: GenerationRequest):\n",
    "    \"\"\"Generate text endpoint\"\"\"\n",
    "    if MODEL is None:  # Check if model is loaded\n",
    "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
    "    \n",
    "    start_time = time.time()        # Start timer\n",
    "    request_id = str(uuid.uuid4())  # Create unique ID for this request\n",
    "    \n",
    "    try:\n",
    "        # Convert user's text to tokens\n",
    "        inputs = TOKENIZER(request.prompt, return_tensors=\"pt\").to(MODEL.device)\n",
    "        \n",
    "        # Generate new tokens\n",
    "        with torch.no_grad():  # Disable gradient calculation (faster)\n",
    "            outputs = MODEL.generate(\n",
    "                **inputs,                     # Pass the tokenized input\n",
    "                max_new_tokens=request.max_tokens,    # How many tokens to make\n",
    "                temperature=request.temperature,      # Randomness level\n",
    "                top_p=request.top_p,                  # Token selection\n",
    "                do_sample=True,                       # Use sampling (not greedy)\n",
    "                pad_token_id=TOKENIZER.eos_token_id   # Padding token ID\n",
    "            )\n",
    "        \n",
    "        # Convert tokens back to text\n",
    "        generated_text = TOKENIZER.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Count how many new tokens were generated\n",
    "        tokens_generated = outputs.shape[1] - inputs.input_ids.shape[1]\n",
    "        # Calculate how long it took\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Return the response in the expected format\n",
    "        return GenerationResponse(\n",
    "            generated_text=generated_text,\n",
    "            request_id=request_id,\n",
    "            tokens_generated=tokens_generated,\n",
    "            inference_time=inference_time\n",
    "        )\n",
    "        \n",
    "    except Exception as e:  # If anything goes wrong\n",
    "        raise HTTPException(status_code=500, detail=f\"Generation failed: {str(e)}\")\n",
    "\n",
    "@app.get(\"/health\")  # GET endpoint at /health\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint - for monitoring\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\", \n",
    "        \"model_loaded\": MODEL is not None,  # Is model loaded?\n",
    "        \"device\": str(MODEL.device) if MODEL else \"none\"  # Where's model running?\n",
    "    }\n",
    "# Run with: uvicorn script_name:app --host 0.0.0.0 --port 8000 --reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d3c321-d321-4b7d-81df-6b02a32241ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf2c0b6e-53bc-4e5d-a52d-0ae1477ddfe4",
   "metadata": {},
   "source": [
    "\n",
    "**In simple terms:** This creates a web server that loads an AI model once, then lets users send text and get AI-generated responses through an API, with proper error handling and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785bbd42-7533-43bc-b42d-c28f14c8e29b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "514224cf-61c9-4634-b837-2fe2d26bdc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from contextlib import asynccontextmanager\n",
    "import uuid\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeeb683c-1c31-47fd-9956-dfba1a813af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request/Response models\n",
    "class GenerationRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_tokens: int = 100\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    stream: bool = False\n",
    "\n",
    "class GenerationResponse(BaseModel):\n",
    "    generated_text: str\n",
    "    request_id: str\n",
    "    tokens_generated: int\n",
    "    inference_time: float\n",
    "\n",
    "# Lifespan manager\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    # Startup\n",
    "    print(\"Loading model...\")\n",
    "    global TOKENIZER, MODEL\n",
    "    \n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    TOKENIZER.pad_token = TOKENIZER.eos_token\n",
    "    \n",
    "    MODEL = AutoModelForCausalLM.from_pretrained(\n",
    "        \"gpt2\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"Model loaded successfully!\")\n",
    "    yield\n",
    "    # Shutdown (optional cleanup)\n",
    "\n",
    "# Initialize FastAPI with lifespan\n",
    "app = FastAPI(title=\"LLM Inference API\", version=\"1.0\", lifespan=lifespan)\n",
    "\n",
    "# Global model and tokenizer\n",
    "MODEL = None\n",
    "TOKENIZER = None\n",
    "\n",
    "@app.post(\"/generate\", response_model=GenerationResponse)\n",
    "async def generate_text(request: GenerationRequest):\n",
    "    if MODEL is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    request_id = str(uuid.uuid4())\n",
    "    \n",
    "    try:\n",
    "        inputs = TOKENIZER(request.prompt, return_tensors=\"pt\").to(MODEL.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = MODEL.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=request.max_tokens,\n",
    "                temperature=request.temperature,\n",
    "                top_p=request.top_p,\n",
    "                do_sample=True,\n",
    "                pad_token_id=TOKENIZER.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = TOKENIZER.decode(outputs[0], skip_special_tokens=True)\n",
    "        tokens_generated = outputs.shape[1] - inputs.input_ids.shape[1]\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        return GenerationResponse(\n",
    "            generated_text=generated_text,\n",
    "            request_id=request_id,\n",
    "            tokens_generated=tokens_generated,\n",
    "            inference_time=inference_time\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Generation failed: {str(e)}\")\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\n",
    "        \"status\": \"healthy\", \n",
    "        \"model_loaded\": MODEL is not None,\n",
    "        \"device\": str(MODEL.device) if MODEL else \"none\"\n",
    "    }\n",
    "# Run with: uvicorn script_name:app --host 0.0.0.0 --port 8000 --reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764b49ef-25b4-4dc8-8a74-99fc761a98dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f3ee60f-e991-4c15-af8c-e5aa415f2957",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e64a4a-c82e-439d-a29a-52f1d9e03f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./distilgpt2-local', './gpt2-local')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'./distilgpt2-local', './gpt2-local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad493be-0006-4e33-b219-782c4cd651c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self, base_url=\"http://localhost:8000\"):\n",
    "        self.base_url = base_url\n",
    "    \n",
    "    def generate(self, prompt, **kwargs):\n",
    "        \"\"\"Sync generation\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/generate\",\n",
    "            json={\"prompt\": prompt, **kwargs}\n",
    "        )\n",
    "        return response.json()\n",
    "    \n",
    "    def generate_stream(self, prompt, **kwargs):\n",
    "        \"\"\"Streaming generation\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/generate-stream\",\n",
    "            json={\"prompt\": prompt, **kwargs},\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                data = line.decode().replace(\"data: \", \"\")\n",
    "                if data.strip():\n",
    "                    yield json.loads(data)\n",
    "\n",
    "# Test client\n",
    "def test_client():\n",
    "    client = LLMClient()\n",
    "    \n",
    "    # Test sync generation\n",
    "    print(\"=== Sync Generation ===\")\n",
    "    result = client.generate(\"The future of AI is\", max_tokens=50)\n",
    "    print(f\"Result: {result['generated_text']}\")\n",
    "    \n",
    "    # Test streaming\n",
    "    print(\"\\n=== Streaming Generation ===\")\n",
    "    for chunk in client.generate_stream(\"Explain quantum computing:\", max_tokens=30):\n",
    "        if not chunk['finished']:\n",
    "            print(chunk['token'], end=\"\", flush=True)\n",
    "    print()\n",
    "\n",
    "# Run client test\n",
    "test_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd716e23-d6ec-460e-899c-0ff1aa0eeaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8642644-d677-4a26-bd59-d292d3ec007d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f23eaa-817c-4171-b528-6c48233e6d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936bbcea-f24b-432a-a50f-5d190f1e1f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
