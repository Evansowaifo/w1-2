{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc4e7af4-f9ef-4913-aca5-69a93c360868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HELLO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48137fd8-8aac-4821-8dc9-8258dcbfba3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120109dc-2b1d-4a4c-bb98-7d82c3540be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare GPT-2 vs LLaMA tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3efc6da-5567-4346-ac62-207ced0e4ff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compare_tokenizers(texts):\n",
    "    gpt2 = AutoTokenizer.from_pretrained('gpt2')\n",
    "    llama =  AutoTokenizer.from_pretrained(\"openlm-research/open_llama_7b\")\n",
    "\n",
    "    for i in texts:\n",
    "        print(f\"\\nOriginal: '{i}'\")\n",
    "        \n",
    "        # GPT-2 (BPE)\n",
    "        gpt2_token = gpt2.encode(i)\n",
    "        gpt_decoded = [gpt2.decode([x]) for x in gpt2_token]\n",
    "        print(f\"gpt token: {gpt2_token}\")\n",
    "        print(f\"gpt decoded: {gpt_decoded}\")\n",
    "\n",
    "        # LLAMA\n",
    "        llama_token = llama.encode(i)\n",
    "        llama_decoded = [llama.decode([x]) for x in llama_token]\n",
    "        print(f\"LLaMA Tokens: {llama_token}\")\n",
    "        print(f\"LLaMA Decoded: {llama_decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00bfc1e6-aff3-4378-9f87-b4a5487aa6b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original: 'Hello world!'\n",
      "gpt token: [15496, 995, 0]\n",
      "gpt decoded: ['Hello', ' world', '!']\n",
      "LLaMA Tokens: [1, 16644, 924, 31905]\n",
      "LLaMA Decoded: ['<s>', 'Hello', 'world', '!']\n",
      "\n",
      "Original: 'I'm learning AI.'\n",
      "gpt token: [40, 1101, 4673, 9552, 13]\n",
      "gpt decoded: ['I', \"'m\", ' learning', ' AI', '.']\n",
      "LLaMA Tokens: [1, 312, 31876, 31836, 3187, 7421, 31843]\n",
      "LLaMA Decoded: ['<s>', 'I', \"'\", 'm', 'learning', 'AI', '.']\n",
      "\n",
      "Original: 'üá≥üá¨ Nigeria'\n",
      "gpt token: [8582, 229, 111, 8582, 229, 105, 19398]\n",
      "gpt decoded: ['ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', ' Nigeria']\n",
      "LLaMA Tokens: [1, 31822, 243, 162, 138, 182, 243, 162, 138, 175, 8700]\n",
      "LLaMA Decoded: ['<s>', '', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'Nigeria']\n",
      "\n",
      "Original: 'def calculate_loss():'\n",
      "gpt token: [4299, 15284, 62, 22462, 33529]\n",
      "gpt decoded: ['def', ' calculate', '_', 'loss', '():']\n",
      "LLaMA Tokens: [1, 918, 15667, 31889, 19388, 20940]\n",
      "LLaMA Decoded: ['<s>', 'def', 'calculate', '_', 'loss', '():']\n",
      "\n",
      "Original: '„Åì„Çì„Å´„Å°„ÅØ'\n",
      "gpt token: [46036, 22174, 28618, 2515, 94, 31676]\n",
      "gpt decoded: ['„Åì', '„Çì', '„Å´', 'ÔøΩ', 'ÔøΩ', '„ÅØ']\n",
      "LLaMA Tokens: [1, 31822, 230, 132, 150, 230, 133, 150, 230, 132, 174, 230, 132, 164, 230, 132, 178]\n",
      "LLaMA Decoded: ['<s>', '', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ']\n",
      "\n",
      "Original: 'I will understand tokenization. Let's figure it out.'\n",
      "gpt token: [40, 481, 1833, 11241, 1634, 13, 3914, 338, 3785, 340, 503, 13]\n",
      "gpt decoded: ['I', ' will', ' understand', ' token', 'ization', '.', ' Let', \"'s\", ' figure', ' it', ' out', '.']\n",
      "LLaMA Tokens: [1, 312, 482, 1738, 13593, 1418, 31843, 2906, 31876, 31829, 4458, 357, 532, 31843]\n",
      "LLaMA Decoded: ['<s>', 'I', 'will', 'understand', 'token', 'ization', '.', 'Let', \"'\", 's', 'figure', 'it', 'out', '.']\n"
     ]
    }
   ],
   "source": [
    "# Test with various texts\n",
    "test_texts = [\n",
    "    \"Hello world!\",\n",
    "    \"I'm learning AI.\",\n",
    "    \"üá≥üá¨ Nigeria\",  # Emoji + text\n",
    "    \"def calculate_loss():\",  # Code\n",
    "    \"„Åì„Çì„Å´„Å°„ÅØ\",  # Japanese\n",
    "    \"I will understand tokenization. Let's figure it out.\"\n",
    "]\n",
    "\n",
    "compare_tokenizers(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d142c34-7b51-4cea-a59a-cfb9a68a0432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28c4d139-0edc-4f87-b13f-fc5aaf42cce9",
   "metadata": {},
   "source": [
    "**Line-by-line:**\n",
    "\n",
    "1. **Define function** `compare_tokenizers` that takes `texts`.\n",
    "2. **Load GPT-2 tokenizer** (BPE method).\n",
    "3. **Load OpenLLaMA tokenizer** (SentencePiece method).\n",
    "4. **Loop** through each text `i`.\n",
    "5. **Print** original text.\n",
    "6. **GPT-2**: Encode text to token IDs.\n",
    "7. **GPT-2**: Decode each token back to string.\n",
    "8. **Print** GPT-2 tokens and decoded strings.\n",
    "9. **LLaMA**: Encode text to token IDs.\n",
    "10. **LLaMA**: Decode each token back to string.\n",
    "11. **Print** LLaMA tokens and decoded strings.\n",
    "\n",
    "Shows how each tokenizer splits and reconstructs text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29e11ed4-3b45-48af-ae41-963734e362fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <!-- NOTE: '<s>' == (start of sentence) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3619e737-3498-41b0-9f09-cc92d63345de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_again(texts):\n",
    "    gpt2 = AutoTokenizer.from_pretrained('gpt2')\n",
    "    llama =  AutoTokenizer.from_pretrained(\"openlm-research/open_llama_7b\")\n",
    "\n",
    "    for i in texts:\n",
    "        print(f\"\\nOriginal: '{i}'\")\n",
    "        \n",
    "        gpt2_token = gpt2.encode(i)\n",
    "        gpt_pieces = gpt2.convert_ids_to_tokens(gpt2_token)\n",
    "        print(f\"gpt token: {gpt2_token}\")\n",
    "        print(f\"gpt pieces: {gpt_pieces}\")\n",
    "        \n",
    "        llama_token = llama.encode(i)\n",
    "        llama_pieces = llama.convert_ids_to_tokens(llama_token)\n",
    "        print(f\"LLaMA Tokens: {llama_token}\")\n",
    "        print(f\"LLaMA Pieces: {llama_pieces}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34260d3d-f474-40ea-aec7-c8ebaca7abb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original: 'Hello world!'\n",
      "gpt token: [15496, 995, 0]\n",
      "gpt pieces: ['Hello', 'ƒ†world', '!']\n",
      "LLaMA Tokens: [1, 16644, 924, 31905]\n",
      "LLaMA Pieces: ['<s>', '‚ñÅHello', '‚ñÅworld', '!']\n",
      "\n",
      "Original: 'I'm learning AI.'\n",
      "gpt token: [40, 1101, 4673, 9552, 13]\n",
      "gpt pieces: ['I', \"'m\", 'ƒ†learning', 'ƒ†AI', '.']\n",
      "LLaMA Tokens: [1, 312, 31876, 31836, 3187, 7421, 31843]\n",
      "LLaMA Pieces: ['<s>', '‚ñÅI', \"'\", 'm', '‚ñÅlearning', '‚ñÅAI', '.']\n",
      "\n",
      "Original: 'üá≥üá¨ Nigeria'\n",
      "gpt token: [8582, 229, 111, 8582, 229, 105, 19398]\n",
      "gpt pieces: ['√∞≈Å', 'ƒ©', '¬≥', '√∞≈Å', 'ƒ©', '¬¨', 'ƒ†Nigeria']\n",
      "LLaMA Tokens: [1, 31822, 243, 162, 138, 182, 243, 162, 138, 175, 8700]\n",
      "LLaMA Pieces: ['<s>', '‚ñÅ', '<0xF0>', '<0x9F>', '<0x87>', '<0xB3>', '<0xF0>', '<0x9F>', '<0x87>', '<0xAC>', '‚ñÅNigeria']\n",
      "\n",
      "Original: 'def calculate_loss():'\n",
      "gpt token: [4299, 15284, 62, 22462, 33529]\n",
      "gpt pieces: ['def', 'ƒ†calculate', '_', 'loss', '():']\n",
      "LLaMA Tokens: [1, 918, 15667, 31889, 19388, 20940]\n",
      "LLaMA Pieces: ['<s>', '‚ñÅdef', '‚ñÅcalculate', '_', 'loss', '():']\n",
      "\n",
      "Original: '„Åì„Çì„Å´„Å°„ÅØ'\n",
      "gpt token: [46036, 22174, 28618, 2515, 94, 31676]\n",
      "gpt pieces: ['√£ƒ£ƒµ', '√£ƒ§ƒµ', '√£ƒ£¬´', '√£ƒ£', '¬°', '√£ƒ£¬Ø']\n",
      "LLaMA Tokens: [1, 31822, 230, 132, 150, 230, 133, 150, 230, 132, 174, 230, 132, 164, 230, 132, 178]\n",
      "LLaMA Pieces: ['<s>', '‚ñÅ', '<0xE3>', '<0x81>', '<0x93>', '<0xE3>', '<0x82>', '<0x93>', '<0xE3>', '<0x81>', '<0xAB>', '<0xE3>', '<0x81>', '<0xA1>', '<0xE3>', '<0x81>', '<0xAF>']\n",
      "\n",
      "Original: 'I will understand tokenization. Let's figure it out.'\n",
      "gpt token: [40, 481, 1833, 11241, 1634, 13, 3914, 338, 3785, 340, 503, 13]\n",
      "gpt pieces: ['I', 'ƒ†will', 'ƒ†understand', 'ƒ†token', 'ization', '.', 'ƒ†Let', \"'s\", 'ƒ†figure', 'ƒ†it', 'ƒ†out', '.']\n",
      "LLaMA Tokens: [1, 312, 482, 1738, 13593, 1418, 31843, 2906, 31876, 31829, 4458, 357, 532, 31843]\n",
      "LLaMA Pieces: ['<s>', '‚ñÅI', '‚ñÅwill', '‚ñÅunderstand', '‚ñÅtoken', 'ization', '.', '‚ñÅLet', \"'\", 's', '‚ñÅfigure', '‚ñÅit', '‚ñÅout', '.']\n"
     ]
    }
   ],
   "source": [
    "# Test with various texts\n",
    "test_texts = [\n",
    "    \"Hello world!\",\n",
    "    \"I'm learning AI.\",\n",
    "    \"üá≥üá¨ Nigeria\",  # Emoji + text\n",
    "    \"def calculate_loss():\",  # Code\n",
    "    \"„Åì„Çì„Å´„Å°„ÅØ\",  # Japanese\n",
    "    \"I will understand tokenization. Let's figure it out.\"\n",
    "]\n",
    "\n",
    "compare_again(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc426a-192d-4f2d-8ebf-0b6023042282",
   "metadata": {},
   "source": [
    "**Line-by-line:**\n",
    "\n",
    "1. **Define function** `compare_again` taking `texts`.\n",
    "2. **Load GPT-2 tokenizer**.\n",
    "3. **Load OpenLLaMA tokenizer**.\n",
    "4. **Loop** through each text `i`.\n",
    "5. **Print** original text.\n",
    "6. **GPT-2**: Encode text to token IDs.\n",
    "7. **GPT-2**: Convert token IDs to subword pieces.\n",
    "8. **Print** GPT-2 tokens and pieces.\n",
    "9. **LLaMA**: Encode text to token IDs.\n",
    "10. **LLaMA**: Convert token IDs to subword pieces.\n",
    "11. **Print** LLaMA tokens and pieces.\n",
    "\n",
    "Shows actual subwords (like `ƒ†` or `<0xE3>`) instead of decoded strings.\n",
    "\n",
    "**ƒ†**: GPT-2's marker for a space before a word.\n",
    "\n",
    "**<0xE3>**: LLaMA's marker for the byte `0xE3` (part of a UTF-8 character).\n",
    "\n",
    "Both show how tokenizers represent raw text internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc82b04-c0db-44b4-b01f-33e7e8e7a145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06612be1-e63d-489c-a2d2-0ef4d426978a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1   ---->   Advanced Tokenization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64816343-6f6f-436f-98f0-9f99b49284bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(texts, model):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "    # Get Tokens and Their Positions\n",
    "    encoding = tokenizer(texts, return_offsets_mapping=True)\n",
    "    tokens = encoding.input_ids\n",
    "    offsets = encoding.offset_mapping\n",
    "    decodedth = [tokenizer.decode([i]) for i in tokens]\n",
    "\n",
    "    print(f\"\\n--- {model} ---\")\n",
    "    print(f\"Text: '{texts}'\")\n",
    "    print(f\"Total offsets: {offsets}\")\n",
    "    print(f\"Total tokens: {tokens}\")\n",
    "    print(f\"Total tokens: {len(tokens)}\")\n",
    "    print(f\"-------------------------------------------------------------------------------------------->\")\n",
    "\n",
    "    # show token --to--> text mapping\n",
    "    for i, (token, (start, end)) in enumerate(zip(tokens, offsets)):\n",
    "        token_text = texts[start:end]\n",
    "        decoded = tokenizer.decode([token])\n",
    "        print(f\"Token {i:2d}: {token:5d} --> '{token_text}' (decode: '{decoded}')\")\n",
    "\n",
    "    return tokens, decodedth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71c682bc-121e-4f58-a285-31365cf142e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- gpt2 ---\n",
      "Text: 'I will understand tokenization. Let's figure it out.'\n",
      "Total offsets: [(0, 1), (1, 6), (6, 17), (17, 23), (23, 30), (30, 31), (31, 35), (35, 37), (37, 44), (44, 47), (47, 51), (51, 52)]\n",
      "Total tokens: [40, 481, 1833, 11241, 1634, 13, 3914, 338, 3785, 340, 503, 13]\n",
      "Total tokens: 12\n",
      "-------------------------------------------------------------------------------------------->\n",
      "Token  0:    40 --> 'I' (decode: 'I')\n",
      "Token  1:   481 --> ' will' (decode: ' will')\n",
      "Token  2:  1833 --> ' understand' (decode: ' understand')\n",
      "Token  3: 11241 --> ' token' (decode: ' token')\n",
      "Token  4:  1634 --> 'ization' (decode: 'ization')\n",
      "Token  5:    13 --> '.' (decode: '.')\n",
      "Token  6:  3914 --> ' Let' (decode: ' Let')\n",
      "Token  7:   338 --> ''s' (decode: ''s')\n",
      "Token  8:  3785 --> ' figure' (decode: ' figure')\n",
      "Token  9:   340 --> ' it' (decode: ' it')\n",
      "Token 10:   503 --> ' out' (decode: ' out')\n",
      "Token 11:    13 --> '.' (decode: '.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([40, 481, 1833, 11241, 1634, 13, 3914, 338, 3785, 340, 503, 13],\n",
       " ['I',\n",
       "  ' will',\n",
       "  ' understand',\n",
       "  ' token',\n",
       "  'ization',\n",
       "  '.',\n",
       "  ' Let',\n",
       "  \"'s\",\n",
       "  ' figure',\n",
       "  ' it',\n",
       "  ' out',\n",
       "  '.'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = \"I will understand tokenization. Let's figure it out.\"\n",
    "\n",
    "model = 'gpt2'\n",
    "# model = \"openlm-research/open_llama_7b\"\n",
    "\n",
    "analyze(texts, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a42d54-6a78-4c96-a2b4-3e6dbac96d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b440e1e-aa95-4716-8625-57a3800a228d",
   "metadata": {},
   "source": [
    "**Line-by-line:**\n",
    "\n",
    "1. **Define function** `analyze` taking `texts` and `model`.\n",
    "2. **Load tokenizer** for given model.\n",
    "3. **Encode text** with `return_offsets_mapping=True` to get character positions.\n",
    "4. **Extract token IDs** and offset mappings.\n",
    "5. **Print** model name.\n",
    "6. **Print** original text.\n",
    "7. **Print** all offsets.\n",
    "8. **Print** all tokens.\n",
    "9. **Print** total token count.\n",
    "10. **Print** separator line.\n",
    "11. **Loop** through each token, its start/end position.\n",
    "12. **Slice** original text to get substring.\n",
    "13. **Decode** token to string.\n",
    "14. **Print** token index, ID, substring, and decoded string.\n",
    "15. **Return** tokens and last decoded string.\n",
    "\n",
    "Shows exact text-to-token mapping with character positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb7222-07e0-481a-a44e-d4712d290268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2ff327f-0816-4d60-8d47-ffd9b37a4598",
   "metadata": {},
   "source": [
    "#### 2 .....  Custom Tokenizer Training (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb0e7a50-4bcd-421f-8631-1a7137819d95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "898e30d5-587f-4672-b8ae-00c952cfa72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple BPE tokenizer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca2770b6-8f29-4c30-9f4d-698096c4adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(texts, vocab_size=1000):\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "    with open('training_text.txt', 'w') as w:\n",
    "        for text in texts:\n",
    "            w.write(text + '\\n')\n",
    "\n",
    "    # Train tokenizer\n",
    "    tokenizer.train(\n",
    "        files = ['training_text.txt'],\n",
    "        vocab_size = vocab_size,\n",
    "        min_frequency = 2,\n",
    "        special_tokens = [\"<|endoftext|>\", \"<|pad|>\", \"<|unk|>\"]\n",
    "    )\n",
    "\n",
    "    # Test it\n",
    "    encoded = tokenizer.encode(\"Hello Sir?\")\n",
    "    print(f\"Custom tokens: {encoded.tokens}\")\n",
    "    print(f\"Custom IDs: {encoded.ids}\")\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14dd826-5587-4149-bbe3-7e503fd0c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on sample data\n",
    "sample = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Hello world! This is a test.\",\n",
    "    \"Machine learning is fascinating.\",\n",
    "    \"Tokenization converts text to numbers.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1225f0a0-9f2d-4109-85a2-80aa05e8d7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom tokens: ['H', 'e', 'l', 'l', 'o', 'ƒ†', 'S', 'i', 'r', '?']\n",
      "Custom IDs: [42, 71, 78, 78, 81, 223, 53, 75, 84, 33]\n"
     ]
    }
   ],
   "source": [
    "custom_tokenizer = train_tokenizer(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97cf6ec6-be4f-436c-8eb1-766914b9399b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(vocabulary_size=274, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)\n"
     ]
    }
   ],
   "source": [
    "print(custom_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b78d3d-4fe7-441d-96ec-0ba2541e820d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a4aaab9-99f2-46aa-9f3d-70e0e1b0e9c4",
   "metadata": {},
   "source": [
    "**Line-by-line:**\n",
    "\n",
    "1. **Define function** `train_tokenizer` with `texts` and `vocab_size` (default 1000).\n",
    "2. **Initialize** a Byte-Pair Encoding (BPE) tokenizer.\n",
    "3. **Open file** `training_text.txt` for writing.\n",
    "4. **Loop** through texts, write each to file.\n",
    "5. **Train tokenizer** on the file:\n",
    "   - `vocab_size`: Target vocabulary size.\n",
    "   - `min_frequency`: Minimum times a token must appear.\n",
    "   - `special_tokens`: Added special tokens.\n",
    "6. **Test tokenizer** on \"Hello Sir?\".\n",
    "7. **Print** resulting subword tokens.\n",
    "8. **Print** their token IDs.\n",
    "9. **Return** trained tokenizer.\n",
    "10. **Define sample text** list.\n",
    "11. **Call function** to train and get `custom_tokenizer`.\n",
    "\n",
    "Trains a new BPE tokenizer from scratch on custom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0922cd-e36c-4fc0-a019-e78b85c6bc40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
