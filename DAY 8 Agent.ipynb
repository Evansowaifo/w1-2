{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43fe701e-1346-47bd-b7fc-d42379653f84",
   "metadata": {},
   "source": [
    "**Purpose of Day 8:** Build real apps, not just generate text. Learn **RAG**, **Agents**, and **Chat systems** to make LLMs useful.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **RAG:** Add your own data/knowledge to LLM.\n",
    "- **Agents:** Make LLM use tools (calculator, search).\n",
    "- **Chat Memory:** Remember conversation history.\n",
    "- **Evaluation:** Measure if your app works well.\n",
    "\n",
    "**You'll code:** Systems that can answer from docs, use tools, and chat with memory. Turns theory into products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e396afbd-928e-488b-8941-8780217a63a3",
   "metadata": {},
   "source": [
    "1.  **RAG System** â€“ Teach LLM your own documents.\n",
    "2.  **LLM Agent** â€“ Give it tools like calculator/weather.\n",
    "3.  **Chat System** â€“ Make a chatbot with memory.\n",
    "4.  **Evaluation** â€“ Measure quality & speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cece76-d130-48eb-89ec-ba52937454d7",
   "metadata": {},
   "source": [
    "#### 0: retiever and own document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08e7cb-47ce-444a-b51d-bc6a5b74124f",
   "metadata": {},
   "source": [
    "**Purpose:** Demonstrate a **RAG (Retrieval-Augmented Generation)** system that lets an LLM answer questions using your own documents as context, not just its training data.\n",
    "\n",
    "**How it works:**\n",
    "1. **`build_knowledge_base`**: Turns your text documents into numerical **embeddings** (vectors) and stores them in a **FAISS vector database** for fast search.\n",
    "2. **`retrieve`**: When you ask a question, it finds the **most relevant documents** from your database using similarity search.\n",
    "3. **`generate_with_context`**: Sends your question **+ the retrieved documents** to the LLM, so it can give an accurate, grounded answer.\n",
    "\n",
    "**Why it matters:** Makes LLMs **factual and specialized** by grounding them in your data. Solves the \"hallucination\" problem for domain-specific questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899593c0-5e39-453c-87ed-a4e5d53c1a0e",
   "metadata": {},
   "source": [
    "**Same purpose and functionality.** Yes.\n",
    "\n",
    "**Difference:** Only **code structure**.\n",
    "\n",
    "| | **Class Version** | **Function Version** |\n",
    "| :--- | :--- | :--- |\n",
    "| **State Storage** | Instance attributes (`self.index`) | Global variables (`faiss_index`) |\n",
    "| **Method Calls** | `rag.retrieve(query)` | `retrieve_documents(query)` |\n",
    "| **LLM Client** | Passed once to `__init__` | Passed each call to `generate_with_context` |\n",
    "| **Result** | Identical RAG system | Identical RAG system |\n",
    "\n",
    "Both:\n",
    "1. Encode documents into vectors.\n",
    "2. Store in FAISS vector database.\n",
    "3. Retrieve relevant docs for a query.\n",
    "4. Generate answer with context.\n",
    "\n",
    "**Choose:** Functions if you prefer simple scripts. Class if you want multiple independent RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "985547d4-a762-41fe-8498-40508ea3ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # math libs\n",
    "from sentence_transformers import SentenceTransformer # convert text to numbers(embeddings)\n",
    "import faiss # vector search library. fast similarity search\n",
    "from typing import List # Type hints for betterr code\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextStreamer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ea5d3d-ed22-41b2-b852-d5026818feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = None    # Will hold the text-to-vector model\n",
    "faiss_index = None        # Will hold the vector database\n",
    "document_list = []        # Will hold original text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe404ae3-f9ac-4a6c-94b7-7f546330e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rag():\n",
    "    global embedding_model  # Use the global variable, not create local one\n",
    "    embedding_model = SentenceTransformer('./all-MiniLM-L6-v2-local', truncate_dim=512)  # Load embedding model\n",
    "    # Make tokenizer pad sequences to same length by reusing the end-of-sequence token. Fixes \"no padding token\" error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d722b2-d85e-403b-9176-39838dbd3bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knowledge_base(documents: List[str]):\n",
    "    global faiss_index, document_list # modify global variables\n",
    "    document_list = documents # store original texts\n",
    "    embeddings = embedding_model.encode(documents) # Convert text documents to number vectors(embeddings), Each document â†’ 384 numbers(vector)\n",
    "    faiss_index = faiss.IndexFlatIP(embeddings.shape[1]) # Create FAISS vector database, \"FlatIP\" = simple index, \"IP\" = Inner Product (similarity measure)\n",
    "    faiss_index.add(embeddings.astype('float32')) # Add all document vectors to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca6f13cd-3a1e-4548-b5aa-50c6b63541d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query: str, k: int=3) -> List[str]:\n",
    "    if faiss_index is None: # check if database exists\n",
    "        return []\n",
    "        \n",
    "    query_embedding = embedding_model.encode([query])# Convert query text to vector (same way as documents)\n",
    "    distances, indices = faiss_index.search(query_embedding.astype('float32'), k) # Search for k most similar vectors in database\n",
    "    # indices[0] = positions of top matches in document_list\n",
    "    return [document_list[i] for i in indices[0] if i < len(document_list)] # Get original text for each matching position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a7efcf-408d-4c67-9fbc-410bb5df9487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(llm, query:str, max_tokens: int=30) -> str:\n",
    "    context_docs = retrieve_documents(query) # Step 1: Get relevant documents for query\n",
    "    context = '\\n'.join(context_docs) # Step 2: Combine documents into context string\n",
    "    \n",
    "    prompt = f\"\"\"Context: {context}\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = llm(prompt, max_new_tokens=max_tokens, temperature=0.3, top_p=0.9) # Step 4: Send to LLM for answer\n",
    "    return response # Step 5: Return LLM's generated answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f1890e-0714-43bd-8d6a-defb20a0aeef",
   "metadata": {},
   "source": [
    "simple analogy:\n",
    "\n",
    "init_rag() = Get a translator (text â†’ numbers)\n",
    "\n",
    "build_knowledge_base() = Index your books (documents â†’ vectors â†’ database)\n",
    "\n",
    "retrieve_documents() = Find relevant book pages for a question\n",
    "\n",
    "generate_with_context() = Give pages + question to expert (LLM) for answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d8cbebf-1469-4651-b5af-fef761a018f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = './gpt2-local'\n",
    "\n",
    "# bnb = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(name, dtype=torch.float16, device_map='cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# streamer = TextStreamer(tokenizer, skip_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49f50814-3736-4cd0-9e08-a8a1d639092a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_with_cache(prompt, max_new_tokens=30, temperature=0.3, top_p=0.9):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        # streamer = streamer,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True, # Internal caching only\n",
    "        repetition_penalty=2.0,       # Strong penalty for repeats\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "\n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # REMOVE everything before \"Answer:\"\n",
    "    if \"Answer:\" in full_text:\n",
    "        return full_text.split(\"Answer:\", 1)[1].strip()\n",
    "    \n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "220d3145-1e70-4675-89d2-41abb372a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RAG - Testing the system step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f15f27c-b67c-48ad-88ad-9c48d2c6c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_rag() # 1. Initialize the embedding model (text â†’ numbers converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c71d60e8-3160-47fa-9dad-64aac9ef533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prepare sample documents (your knowledge base)\n",
    "documents = [\n",
    "    \"Transformers are neural networks that use self-attention mechanisms.\",\n",
    "    \"GPT-4 can process both text and images through its multimodal architecture.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e609091-408b-4403-a4fa-d9e039852bf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3. Build the vector database from documents\n",
    "build_knowledge_base(documents) # Converts texts to vectors, stores in FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61f10d71-7a80-49a3-a692-8ccf03e97965",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are transformers?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d911a138-f3f1-44a8-9b69-755631b7ed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved: ['Transformers are neural networks that use self-attention mechanisms.', 'GPT-4 can process both text and images through its multimodal architecture.', 'GPT-4 can process both text and images through its multimodal architecture.']\n"
     ]
    }
   ],
   "source": [
    "# 5. Retrieve relevant documents from database\n",
    "results = retrieve_documents(query)  # Finds documents about \"transformers\" using vector similarity\n",
    "\n",
    "print(f\"Retrieved: {results}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "223d30e7-b89d-405b-9206-ade5978332f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Transformators allow for the processing of multiple information at once, which is a very important feature in any computer program (see below). The most common form\n"
     ]
    }
   ],
   "source": [
    "# 6. Generate answer using retrieved context\n",
    "answer = generate(generate_with_cache, query)  # Combines found docs + question â†’ sends to MockLLM â†’ gets fake answer\n",
    "\n",
    "print(f\"Answer: {answer}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988d132-9036-4d45-a024-d244908236d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99034caf-1050-4a43-8054-5ee52cb396b5",
   "metadata": {},
   "source": [
    "What's happening:\n",
    "\n",
    "Mock client simulates an LLM (returns question back as \"answer\")\n",
    "\n",
    "Real RAG processes documents â†’ vector DB â†’ similarity search\n",
    "\n",
    "Test shows RAG finds right documents, passes them to \"LLM\"\n",
    "\n",
    "Output proves RAG pipeline works (ready for real LLM like GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1e31aa-f808-490d-b472-0443b66b0f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01b7a2af-acee-4a57-bc3e-b974584665db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import faiss\n",
    "# from typing import List\n",
    "\n",
    "# # Global state\n",
    "# embedding_model = None\n",
    "# faiss_index = None\n",
    "# document_list = []\n",
    "\n",
    "# def init_rag():\n",
    "#     global embedding_model\n",
    "#     embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# def build_knowledge_base(documents: List[str]):\n",
    "#     global faiss_index, document_list\n",
    "#     document_list = documents\n",
    "#     embeddings = embedding_model.encode(documents)\n",
    "#     faiss_index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "#     faiss_index.add(embeddings.astype('float32'))\n",
    "\n",
    "# def retrieve_documents(query: str, k: int = 3) -> List[str]:\n",
    "#     if faiss_index is None:\n",
    "#         return []\n",
    "#     query_embedding = embedding_model.encode([query])\n",
    "#     distances, indices = faiss_index.search(query_embedding.astype('float32'), k)\n",
    "#     return [document_list[i] for i in indices[0] if i < len(document_list)]\n",
    "\n",
    "# def generate_with_context(llm_client, query: str, max_tokens: int = 150) -> str:\n",
    "#     context_docs = retrieve_documents(query)\n",
    "#     context = \"\\n\".join(context_docs)\n",
    "#     prompt = f\"\"\"Based on the following context, answer the question.\n",
    "\n",
    "# Context:\n",
    "# {context}\n",
    "\n",
    "# Question: {query}\n",
    "\n",
    "# Answer:\"\"\"\n",
    "#     response = llm_client.generate(prompt, max_tokens=max_tokens)\n",
    "#     return response[\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c42b7c-0f1a-4b89-b937-b5ceddbb3915",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Mock LLM client\n",
    "# class MockLLMClient:\n",
    "#     def generate(self, prompt, **kwargs):\n",
    "#         return {\"generated_text\": f\"Answer about: {prompt.split('Question:')[-1].strip()}\"}\n",
    "\n",
    "# # Run RAG\n",
    "# init_rag()\n",
    "# documents = [\"Transformers are neural networks...\", \"GPT-4 can process both text...\"]\n",
    "# build_knowledge_base(documents)\n",
    "\n",
    "# query = \"What are transformers in AI?\"\n",
    "# results = retrieve_documents(query)\n",
    "# print(f\"Retrieved: {results}\")\n",
    "# answer = generate_with_context(MockLLMClient(), query)\n",
    "# print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca71c0-e506-418b-9962-9dc8cf4e3e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cacef3a-b28e-4726-97e8-12710a65546c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1: Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20f225b-4400-4584-9595-61052ad48591",
   "metadata": {},
   "source": [
    "**Purpose:** Create an **LLM Agent** that can **use tools** (calculator, weather, time) to complete real-world tasks, not just generate text.\n",
    "\n",
    "**How it works:**\n",
    "1. **Agent has tools** â€“ Functions it can call (calculator, weather lookup, time).\n",
    "2. **LLM decides** â€“ Given a query, LLM decides if/how to use tools.\n",
    "3. **Uses tools** â€“ Agent calls the tool, gets result.\n",
    "4. **Returns answer** â€“ LLM combines tool result with original query for final answer.\n",
    "\n",
    "**Example:**\n",
    "- **Query:** `\"What's 15 * 8 + 3?\"`\n",
    "- **Agent:** Uses calculator tool â†’ gets `123` â†’ returns `\"The answer is 123.\"`\n",
    "\n",
    "**Why it matters:** Turns LLM from a **text generator** into an **action taker** that can interact with the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa0e15-2e71-429f-b07e-a7d3a0020805",
   "metadata": {},
   "source": [
    "**Same purpose and functionality.** Yes.\n",
    "\n",
    "**Difference:** Only **code structure**.\n",
    "\n",
    "| | **Class Version** | **Function Version** |\n",
    "| :--- | :--- | :--- |\n",
    "| **State** | Instance attributes (`self.tools`) | Global variables (`tools`) |\n",
    "| **Method Calls** | `agent.process_query(query)` | `process_query(query)` |\n",
    "| **Tool Definition** | Methods inside class (`def get_weather`) | Standalone functions |\n",
    "| **Setup** | `agent = LLMAgent(client)` | `init_agent(client)` |\n",
    "| **Result** | Identical agent system | Identical agent system |\n",
    "\n",
    "Both:\n",
    "1. Define tools (weather, calculator, time).\n",
    "2. LLM decides when to use tools.\n",
    "3. Parse LLM's tool choice.\n",
    "4. Execute tool and generate final answer.\n",
    "\n",
    "**Choose:** Functions for simple scripts, class for multiple independent agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca0b6e5-fada-4dc7-9c48-ae747f0f89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # Regular expressions for parsing text\n",
    "import requests  # HTTP requests (not used here but typically for API tools)\n",
    "from datetime import datetime  # Get current time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e689ecc3-b49a-4a4b-ab7c-c2a76ac49be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = None  # Will hold the LLM (GPT-2, etc.)\n",
    "tools = {}  # Dictionary to store available tools\n",
    "conversation_history = []  # Track conversation (not used here but available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e68e64-e876-4aa0-bab0-d48a0b67729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf613434-0c1d-44a9-ad1a-d655d813dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool functions - the actual capabilities the agent can use\n",
    "def get_weather(city):\n",
    "    weather_data = {\n",
    "        \"london\": \"Sunny, 15Â°C\",\n",
    "        \"new york\": \"Cloudy, 12Â°C\", \n",
    "        \"tokyo\": \"Rainy, 98Â°C\",\n",
    "        \"lagos\": \"Sunny, 28Â°C\"\n",
    "    }\n",
    "    \n",
    "    city_lower = city.lower()\n",
    "    \n",
    "    if city_lower in weather_data:\n",
    "        return weather_data[city_lower]\n",
    "    else:\n",
    "        return \"Weather data not available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00212af3-4f0e-475b-953d-8dddc8e860ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(expression: str) -> str:\n",
    "    try:\n",
    "        # Safe math evaluation (no builtins for security)\n",
    "        result = eval(expression, {\"__builtins__\": {}})\n",
    "        return str(result) \n",
    "    except:\n",
    "        return \"Error calculating expression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58fcbfac-94f6-457f-becc-3df3cbfd82b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(city: str) -> str:\n",
    "    # Mock time data for different timezones\n",
    "    times = {\n",
    "        \"london\": \"GMT: \" + datetime.utcnow().strftime(\"%H:%M\"),\n",
    "        \"new york\": \"EST: \" + datetime.utcnow().strftime(\"%H:%M\"),\n",
    "        \"tokyo\": \"JST: \" + datetime.utcnow().strftime(\"%H:%M\"),\n",
    "        \"lagos\": \"WAT: \" + datetime.utcnow().strftime(\"%H:%M\")\n",
    "    }\n",
    "    return times.get(city.lower(), \"Timezone not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0edc5bb-b034-4d79-9730-b5c7d7f9ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f43d2da2-4a0d-4771-950f-0c9ab67d0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_agent(client): # Agent functions - manage the agent system\n",
    "    global llm_client # Modify global variable\n",
    "    llm_client = client # Store the LLM client for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b41b725f-ada3-4622-9d37-449d46004ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tool(tool_name, tool_function, description):\n",
    "     # Add a tool to the global tools dictionary\n",
    "    tools[tool_name] = {\n",
    "        'function':tool_function,   # The actual function to call\n",
    "        'description':description # How to describe it to the LLM\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99fedaf8-5551-460a-86f4-a4906de8b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    if llm_client is None:\n",
    "        return \"Agent not initialized\"\n",
    "\n",
    "    # Build tools description for LLM prompt\n",
    "    tools_desc = ''\n",
    "    for name, tool in tools.items():\n",
    "        tools_desc += f\"- {name}: {tool['description']}\\n\"\n",
    "\n",
    "    # Create prompt that tells LLM about available tools\n",
    "    prompt = f\"\"\"STRICT INSTRUCTIONS: For weather, time, or math questions, you MUST output in this exact format:\n",
    "\n",
    "THOUGHT: User asks about [topic].\n",
    "TOOL: [tool_name]\n",
    "INPUT: [input]\n",
    "\n",
    "Available tools: get_weather, get_time, calculate.\n",
    "\n",
    "User: {query}\n",
    "\n",
    "Your response:\"\"\"\n",
    "\n",
    "    # Get LLM's decision\n",
    "    response = llm_client(prompt, max_tokens=50)\n",
    "    llm_output = response[\"generated_text\"]\n",
    "    \n",
    "    # Parse tool use if LLM chose to use one\n",
    "    if \"TOOL:\" in llm_output:\n",
    "        # Extract tool name using regex\n",
    "        tool_match = re.search(r\"TOOL:\\s*(\\w+)\", llm_output)\n",
    "        # Extract input for the tool\n",
    "        input_match = re.search(r\"INPUT:\\s*(.+)\", llm_output)\n",
    "        \n",
    "        if tool_match and input_match:\n",
    "            tool_name = tool_match.group(1)  # e.g., \"get_weather\"\n",
    "            tool_input = input_match.group(1).strip()  # e.g., \"Tokyo\"\n",
    "            \n",
    "            if tool_name in tools:\n",
    "                # Actually call the tool function\n",
    "                tool_result = tools[tool_name]['function'](tool_input)\n",
    "                \n",
    "                # Create final prompt with tool result\n",
    "                final_prompt = f\"\"\"Original query: {query}\n",
    "Tool used: {tool_name} with input: {tool_input}\n",
    "Tool result: {tool_result}\n",
    "\n",
    "Provide helpful final answer:\"\"\"\n",
    "                \n",
    "                # Get final answer from LLM\n",
    "                final_response = llm_client(final_prompt, max_tokens=50)\n",
    "                return final_response[\"generated_text\"]\n",
    "    \n",
    "    return llm_output  # If no tool used, return LLM's direct response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44e5d314-3c57-4d1a-8519-f405fa4c709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================================================= #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "322c9103-81ee-4df5-b69b-a0008fe19b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llama(prompt, max_tokens=50):\n",
    "    # API URL (new router system)\n",
    "    API_URL = \"https://router.huggingface.co/v1/chat/completions\"\n",
    "    \n",
    "    # Headers with your REAL token\n",
    "    headers = {\"Authorization\": \"Bearer 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\"}  # Replace with your token\n",
    "    \n",
    "    # Payload with model and parameters\n",
    "    payload = {\n",
    "        \"model\": \"Qwen/Qwen2.5-Coder-7B-Instruct\",  # Also available\n",
    "        # \"model\": \"meta-llama/Llama-3.1-8b-instruct\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.1  # Makes output more consistent\n",
    "    }\n",
    "    \n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    response_data = response.json()\n",
    "    \n",
    "    try:\n",
    "        return {\"generated_text\": response_data['choices'][0]['message']['content']}\n",
    "    except KeyError:\n",
    "        return {\"generated_text\": f\"API Error: {response_data}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ff67c30-ab90-4b24-98ff-ad774b69e110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': '200'}\n"
     ]
    }
   ],
   "source": [
    "print(query_llama(\"100+150/5. Answer only\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eee92cf9-b69a-4a56-82e2-15ef8581bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================++++++++++================================================================= #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a495032f-b7fe-4e9e-8229-394075f02fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_agent(query_llama)  # Initialize with fake LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4d34312f-53c1-48b7-b8ee-aaa9a01cea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_tool(\"get_weather\", get_weather, \"Get weather for a city\")\n",
    "add_tool(\"calculate\", calculate, \"Calculate math expressions\")\n",
    "add_tool(\"get_time\", get_time, \"Get current time for major cities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f029ee6f-0895-4037-8dd3-20d2a890a5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Agent System Test ===\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Agent System Test ===\")\n",
    "queries = [\n",
    "    \"What's the weather in okland?\",\n",
    "    \"Calculate 15 * 8 + 3\",\n",
    "    \"What time is it in Denmark?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2a69ca18-d115-4045-8626-e8d0f6363e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What's the weather in okland?\n",
      "Response: I apologize for the inconvenience. It appears that there was an error in retrieving the weather data for Oklahoma. Please try asking the question again or specify a more precise location within Oklahoma if possible. I'll do my best to assist you with accurate weather information\n",
      "\n",
      "Query: Calculate 15 * 8 + 3\n",
      "Response: The final answer is 123.\n",
      "\n",
      "Query: What time is it in Denmark?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HELLO!\\AppData\\Local\\Temp\\ipykernel_9212\\1371204806.py:4: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"london\": \"GMT: \" + datetime.utcnow().strftime(\"%H:%M\"),\n",
      "C:\\Users\\HELLO!\\AppData\\Local\\Temp\\ipykernel_9212\\1371204806.py:5: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"new york\": \"EST: \" + datetime.utcnow().strftime(\"%H:%M\"),\n",
      "C:\\Users\\HELLO!\\AppData\\Local\\Temp\\ipykernel_9212\\1371204806.py:6: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"tokyo\": \"JST: \" + datetime.utcnow().strftime(\"%H:%M\"),\n",
      "C:\\Users\\HELLO!\\AppData\\Local\\Temp\\ipykernel_9212\\1371204806.py:7: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"lagos\": \"WAT: \" + datetime.utcnow().strftime(\"%H:%M\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: I apologize for the inconvenience. It seems that the tool I used to retrieve the current time in Denmark did not provide the expected information. However, I can help you find the correct time zone and time for Denmark.\n",
      "\n",
      "Denmark operates on Central European Time\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test each query\n",
    "for query in queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    response = process_query(query)\n",
    "    print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6014116-df17-4ebb-a816-2e783bb33334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14c81c36-fb94-485e-a61f-e94a28cfbc80",
   "metadata": {},
   "source": [
    "**`process_query()` is the agent.**\n",
    "\n",
    "It:\n",
    "1. Takes user query\n",
    "2. Asks LLM if tool needed\n",
    "3. Parses LLM's `TOOL:` response\n",
    "4. Calls the tool function\n",
    "5. Returns final answer\n",
    "\n",
    "**Other parts:**\n",
    "- `get_weather`, `get_time` â†’ **tools**\n",
    "- `add_tool()` â†’ **registers tools**\n",
    "- `process_query()` the agent.\n",
    "- `init_agent()` â†’ **sets up agent**\n",
    "- `mock()` â†’ **LLM backend**\n",
    "\n",
    "The **agent** coordinates everything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9895a0b9-ca37-40ef-a105-26a74ed8e94e",
   "metadata": {},
   "source": [
    "**Free cloud APIs:**\n",
    "\n",
    "1. **Hugging Face Inference API** â€” free tier, rate limited\n",
    "2. **Groq** â€” free for Llama 3.1 70B (fast, but may require sign-up)\n",
    "3. **Together.ai** â€” free credits initially\n",
    "4. **Perplexity API** â€” free tier limited\n",
    "5. **Cohere** â€” trial credits\n",
    "\n",
    "**Best truly free:** **Hugging Face** â€” no credit card, just token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f1d5df-8e47-407a-89c1-7191869bd119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8bde0ce-b2e5-4e92-97b3-b04f1965f38d",
   "metadata": {},
   "source": [
    "#### 2 `Chat System with Memory`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d29aa18-517b-45ff-9c56-1f9d1789392a",
   "metadata": {},
   "source": [
    "**`Chat System - Functional Version`**\n",
    "\n",
    "* A **comment**.\n",
    "* Explains this is a **chat system written using functions**, not a class.\n",
    "\n",
    "---\n",
    "\n",
    "## `def init_chat_system(llm_client, system_prompt=None):`\n",
    "\n",
    "* Defines a **function** named `init_chat_system`.\n",
    "* Purpose: **create and return a chat system object**.\n",
    "* `llm_client` â†’ a function that talks to an LLM.\n",
    "* `system_prompt=None` â†’ optional system instruction.\n",
    "\n",
    "---\n",
    "\n",
    "## `\"\"\"Initialize a chat system (replaces __init__)\"\"\"`\n",
    "\n",
    "* A **docstring**.\n",
    "* Explains this function plays the role of a class `__init__`.\n",
    "\n",
    "---\n",
    "\n",
    "## `return {`\n",
    "\n",
    "* Returns a **dictionary** that stores chat state.\n",
    "\n",
    "---\n",
    "\n",
    "### `\"llm_client\": llm_client,`\n",
    "\n",
    "* Stores the LLM function.\n",
    "* Used later to generate responses.\n",
    "\n",
    "---\n",
    "\n",
    "### `\"conversation_history\": [],`\n",
    "\n",
    "* Starts an **empty list**.\n",
    "* Will store all messages (user + assistant).\n",
    "\n",
    "---\n",
    "\n",
    "### `\"system_prompt\": system_prompt or \"You are a helpful AI assistant.\"`\n",
    "\n",
    "* If `system_prompt` is provided â†’ use it.\n",
    "* Otherwise â†’ default system instruction.\n",
    "* Ensures the system **always has guidance**.\n",
    "\n",
    "---\n",
    "\n",
    "## `}`\n",
    "\n",
    "* Ends the dictionary.\n",
    "\n",
    "---\n",
    "\n",
    "## `def add_message(chat_system, role: str, content: str):`\n",
    "\n",
    "* Defines a function to **add messages to history**.\n",
    "* `chat_system` â†’ the dictionary created earlier.\n",
    "* `role` â†’ `\"user\"` or `\"assistant\"`.\n",
    "* `content` â†’ the message text.\n",
    "\n",
    "---\n",
    "\n",
    "## `\"\"\"Add message to conversation history\"\"\"`\n",
    "\n",
    "* Explains the functionâ€™s purpose.\n",
    "\n",
    "---\n",
    "\n",
    "## `chat_system[\"conversation_history\"].append({\"role\": role, \"content\": content})`\n",
    "\n",
    "* Appends a new message.\n",
    "* Stored as:\n",
    "\n",
    "  ```python\n",
    "  {\"role\": \"user\", \"content\": \"Hello\"}\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## `# Keep only last 10 messages to manage context`\n",
    "\n",
    "* Comment explaining why the next code exists.\n",
    "* Prevents prompts from becoming too long.\n",
    "\n",
    "---\n",
    "\n",
    "## `if len(chat_system[\"conversation_history\"]) > 10:`\n",
    "\n",
    "* Checks if more than **10 messages** exist.\n",
    "\n",
    "---\n",
    "\n",
    "## `chat_system[\"conversation_history\"] = chat_system[\"conversation_history\"][-10:]`\n",
    "\n",
    "* Keeps **only the last 10 messages**.\n",
    "* Older messages are discarded.\n",
    "\n",
    "---\n",
    "\n",
    "## `def format_conversation(chat_system) -> str:`\n",
    "\n",
    "* Function that **builds a prompt** for the LLM.\n",
    "* `-> str` means it returns a **string**.\n",
    "\n",
    "---\n",
    "\n",
    "## `\"\"\"Format conversation for LLM\"\"\"`\n",
    "\n",
    "* Explains purpose.\n",
    "\n",
    "---\n",
    "\n",
    "## `formatted = f\"System: {chat_system['system_prompt']}\\n\\n\"`\n",
    "\n",
    "* Starts the prompt with a **system message**.\n",
    "* `\\n\\n` adds spacing for readability.\n",
    "\n",
    "---\n",
    "\n",
    "## `for msg in chat_system[\"conversation_history\"]:`\n",
    "\n",
    "* Loops through every stored message.\n",
    "\n",
    "---\n",
    "\n",
    "## `formatted += f\"{msg['role'].capitalize()}: {msg['content']}\\n\"`\n",
    "\n",
    "* Adds each message to the prompt.\n",
    "* Capitalizes role:\n",
    "\n",
    "  * `user` â†’ `User`\n",
    "  * `assistant` â†’ `Assistant`\n",
    "\n",
    "---\n",
    "\n",
    "## `formatted += \"Assistant:\"`\n",
    "\n",
    "* Tells the LLM:\n",
    "  ðŸ‘‰ â€œNow YOU speak.â€\n",
    "* Very important for chat models.\n",
    "\n",
    "---\n",
    "\n",
    "## `return formatted`\n",
    "\n",
    "* Returns the final prompt string.\n",
    "\n",
    "---\n",
    "\n",
    "## `def chat(chat_system, user_message: str) -> str:`\n",
    "\n",
    "* Main chat function.\n",
    "* Takes a **user message**.\n",
    "* Returns an **assistant response**.\n",
    "\n",
    "---\n",
    "\n",
    "## `\"\"\"Generate response maintaining conversation context\"\"\"`\n",
    "\n",
    "* Explains what this function does.\n",
    "\n",
    "---\n",
    "\n",
    "## `add_message(chat_system, \"user\", user_message)`\n",
    "\n",
    "* Saves the userâ€™s message to history.\n",
    "\n",
    "---\n",
    "\n",
    "## `prompt = format_conversation(chat_system)`\n",
    "\n",
    "* Builds the full conversation prompt.\n",
    "\n",
    "---\n",
    "\n",
    "## `response = chat_system[\"llm_client\"](prompt, max_tokens=150)`\n",
    "\n",
    "* Calls the LLM.\n",
    "* Sends:\n",
    "\n",
    "  * The full prompt\n",
    "  * Token limit\n",
    "* Gets AI output.\n",
    "\n",
    "---\n",
    "\n",
    "## `assistant_response = response[\"generated_text\"].strip()`\n",
    "\n",
    "* Extracts text from the LLM response.\n",
    "* `.strip()` removes extra spaces/newlines.\n",
    "\n",
    "---\n",
    "\n",
    "## `add_message(chat_system, \"assistant\", assistant_response)`\n",
    "\n",
    "* Saves the AIâ€™s reply into history.\n",
    "\n",
    "---\n",
    "\n",
    "## `return assistant_response`\n",
    "\n",
    "* Sends the reply back to the user.\n",
    "\n",
    "---\n",
    "\n",
    "## `# Mock LLM client for testing`\n",
    "\n",
    "* Comment.\n",
    "* Explains the next function is **fake**.\n",
    "\n",
    "---\n",
    "\n",
    "## `def mock_llm_client(prompt, **kwargs):`\n",
    "\n",
    "* Fake LLM function.\n",
    "* Accepts:\n",
    "\n",
    "  * `prompt`\n",
    "  * Any extra arguments (`**kwargs`).\n",
    "\n",
    "---\n",
    "\n",
    "## `\"\"\"Simple mock LLM for testing\"\"\"`\n",
    "\n",
    "* Explains this function simulates an LLM.\n",
    "\n",
    "---\n",
    "\n",
    "## `return {\"generated_text\": f\"Mock response to: {prompt[:50]}...\"}`\n",
    "\n",
    "* Returns a fake response.\n",
    "* Shows only first 50 characters of the prompt.\n",
    "\n",
    "---\n",
    "\n",
    "## `def test_chat_system():`\n",
    "\n",
    "* Defines a test function.\n",
    "\n",
    "---\n",
    "\n",
    "## `\"\"\"Test the functional chat system\"\"\"`\n",
    "\n",
    "* Explains purpose.\n",
    "\n",
    "---\n",
    "\n",
    "## `chat_sys = init_chat_system(mock_llm_client, \"You are a knowledgeable AI tutor.\")`\n",
    "\n",
    "* Creates a chat system.\n",
    "* Uses:\n",
    "\n",
    "  * Fake LLM\n",
    "  * Custom system prompt.\n",
    "\n",
    "---\n",
    "\n",
    "## `print(\"=== Functional Chat System Test ===\")`\n",
    "\n",
    "* Prints a test header.\n",
    "\n",
    "---\n",
    "\n",
    "## `messages = [ ... ]`\n",
    "\n",
    "* A list of user messages to simulate a conversation.\n",
    "\n",
    "---\n",
    "\n",
    "## `for msg in messages:`\n",
    "\n",
    "* Loops through each test message.\n",
    "\n",
    "---\n",
    "\n",
    "## `print(f\"User: {msg}\")`\n",
    "\n",
    "* Prints user message.\n",
    "\n",
    "---\n",
    "\n",
    "## `response = chat(chat_sys, msg)`\n",
    "\n",
    "* Sends message to chat system.\n",
    "* Gets AI reply.\n",
    "\n",
    "---\n",
    "\n",
    "## `print(f\"Assistant: {response}\\n\")`\n",
    "\n",
    "* Prints AI reply with spacing.\n",
    "\n",
    "---\n",
    "\n",
    "## `if __name__ == \"__main__\":`\n",
    "\n",
    "* Checks if file is **run directly**.\n",
    "\n",
    "---\n",
    "\n",
    "## `test_chat_system()`\n",
    "\n",
    "* Runs the test **only when executed directly**.\n",
    "* Does **not run when imported**.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Big Picture**\n",
    "\n",
    "This code builds:\n",
    "\n",
    "* A **stateful chat system**\n",
    "* Without classes\n",
    "* With conversation memory\n",
    "* Ready to swap in a real LLM\n",
    "* One step away from an **agent**\n",
    "\n",
    "If you want, I can:\n",
    "\n",
    "* Convert this to an **agent**\n",
    "* Add **tool use**\n",
    "* Turn it into a **class-based system**\n",
    "* Replace mock LLM with **LLaMA / HF / OpenAI**\n",
    "\n",
    "Just say which.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d293df1-12b5-4d82-bf7f-9aefb99c27f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576ac7db-0c30-4ddf-8c5a-f866c3bdcf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store\n",
    "def init_chat_system(llm_client, system_prompt=None):\n",
    "    return {\n",
    "        'llm_client' : llm_client,\n",
    "        'conversation_history' : [],\n",
    "        'system_prompt' : system_prompt or \"You are a helpful AI assistant.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f226996c-d0c6-4c0d-a0d5-cb93b26683e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add chat history\n",
    "def add_message(chat_system, role, content):\n",
    "    chat_system['conversation_history'].append({'role': role, 'content': content})\n",
    "    if len(chat_system['conversation_history']) > 10:\n",
    "        chat_system['conversation_history'] = chat_system['conversation_history'][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb8f79ac-bafa-4dbb-990b-8edfa95a65a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arranging chat history\n",
    "def format_conversation(chat_system):\n",
    "    formatted = f\"System: {chat_system['system_prompt']}\\n\\n\"\n",
    "    \n",
    "    for msg in chat_system[\"conversation_history\"]:\n",
    "        formatted += f\"{msg['role'].capitalize}: {msg['content']}\\n\"\n",
    "\n",
    "    formatted += 'Assistant:'\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a45c74bd-cea0-4bdd-b29b-0fd909d582da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(chat_system, user_message):\n",
    "    add_message(chat_system, 'user', user_message)\n",
    "    prompt = format_conversation(chat_system)\n",
    "    response = chat_system['llm_client'](prompt, max_tokens=50)\n",
    "    assistant_response = response['generated_text'].strip()\n",
    "    add_message(chat_system, 'assistant', assistant_response)\n",
    "    return assistant_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8388f27-7336-41a6-ba43-3e4a35dcecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llama(prompt, max_tokens=50):\n",
    "    # API URL (new router system)\n",
    "    API_URL = \"https://router.huggingface.co/v1/chat/completions\"\n",
    "    \n",
    "    # Headers with your REAL token\n",
    "    headers = {\"Authorization\": \"Bearer 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\"}  # Replace with your token\n",
    "    \n",
    "    # Payload with model and parameters\n",
    "    payload = {\n",
    "        \"model\": \"Qwen/Qwen2.5-Coder-7B-Instruct\",  # Also available\n",
    "        # \"model\": \"meta-llama/Llama-3.1-8b-instruct\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.1  # Makes output more consistent\n",
    "    }\n",
    "    \n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    response_data = response.json()\n",
    "    \n",
    "    try:\n",
    "        return {\"generated_text\": response_data['choices'][0]['message']['content']}\n",
    "    except KeyError:\n",
    "        return {\"generated_text\": f\"API Error: {response_data}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9152d35-df68-4d2d-94cd-2504c923cfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_sys = init_chat_system(query_llama, \"You are a knowledgeable AI tutor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e23e7c1f-4a11-48e4-b05e-cb0948dac660",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"Hello! Can you help me learn about machine learning?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9242551a-c76b-4384-8eba-79469cfad86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_sys['conversation_history'][2]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5399bb08-4b1e-49e1-8952-522ee679a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "respond = chat(chat_sys, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f29c5b18-51a3-433b-ae28-210e68eda0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Error: {'error': 'User Access Token \"agent47\" is expired'}\n"
     ]
    }
   ],
   "source": [
    "print(respond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7e72257-e01d-4a8c-89fe-ba5741f47cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"which part is about computer vision?\"\n",
    "respond = chat(chat_sys, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f50aa8f7-16d9-410c-af6e-f41a51a2361d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer vision is not directly related to the core concept of machine learning as defined above. However, it can be considered an application or a field within machine learning. Computer vision focuses on enabling machines to interpret and understand visual information from the world around them,\n"
     ]
    }
   ],
   "source": [
    "print(respond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af924eab-4a15-40f6-a9ef-ec1c5d399d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9039fbca-0acd-4580-8e93-f0b369532bd7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Chat System - Functional Version\n",
    "# def init_chat_system(llm_client, system_prompt=None):\n",
    "#     \"\"\"Initialize a chat system (replaces __init__)\"\"\"\n",
    "#     return {\n",
    "#         \"llm_client\": llm_client,\n",
    "#         \"conversation_history\": [],\n",
    "#         \"system_prompt\": system_prompt or \"You are a helpful AI assistant.\"\n",
    "#     }\n",
    "\n",
    "# def add_message(chat_system, role: str, content: str):\n",
    "#     \"\"\"Add message to conversation history\"\"\"\n",
    "#     chat_system[\"conversation_history\"].append({\"role\": role, \"content\": content})\n",
    "    \n",
    "#     # Keep only last 10 messages to manage context\n",
    "#     if len(chat_system[\"conversation_history\"]) > 10:\n",
    "#         chat_system[\"conversation_history\"] = chat_system[\"conversation_history\"][-10:]\n",
    "\n",
    "# def format_conversation(chat_system) -> str:\n",
    "#     \"\"\"Format conversation for LLM\"\"\"\n",
    "#     formatted = f\"System: {chat_system['system_prompt']}\\n\\n\"\n",
    "    \n",
    "#     for msg in chat_system[\"conversation_history\"]:\n",
    "#         formatted += f\"{msg['role'].capitalize()}: {msg['content']}\\n\"\n",
    "        \n",
    "#     formatted += \"Assistant:\"\n",
    "#     return formatted\n",
    "\n",
    "# def chat(chat_system, user_message: str) -> str:\n",
    "#     \"\"\"Generate response maintaining conversation context\"\"\"\n",
    "#     add_message(chat_system, \"user\", user_message)\n",
    "    \n",
    "#     prompt = format_conversation(chat_system)\n",
    "#     # Note: Changed .generate() to direct call to match your llm_client\n",
    "#     response = chat_system[\"llm_client\"](prompt, max_tokens=150)\n",
    "    \n",
    "#     assistant_response = response[\"generated_text\"].strip()\n",
    "#     add_message(chat_system, \"assistant\", assistant_response)\n",
    "    \n",
    "#     return assistant_response\n",
    "\n",
    "# # Mock LLM client for testing\n",
    "# def mock_llm_client(prompt, **kwargs):\n",
    "#     \"\"\"Simple mock LLM for testing\"\"\"\n",
    "#     return {\"generated_text\": f\"Mock response to: {prompt[:50]}...\"}\n",
    "\n",
    "# # Test the functional chat system\n",
    "# def test_chat_system():\n",
    "#     \"\"\"Test the functional chat system\"\"\"\n",
    "#     # Initialize chat system\n",
    "#     chat_sys = init_chat_system(mock_llm_client, \"You are a knowledgeable AI tutor.\")\n",
    "    \n",
    "#     print(\"=== Functional Chat System Test ===\")\n",
    "#     messages = [\n",
    "#         \"Hello! Can you help me learn about machine learning?\",\n",
    "#         \"What's the difference between supervised and unsupervised learning?\",\n",
    "#         \"Can you give me an example of each?\"\n",
    "#     ]\n",
    "    \n",
    "#     for msg in messages:\n",
    "#         print(f\"User: {msg}\")\n",
    "#         response = chat(chat_sys, msg)\n",
    "#         print(f\"Assistant: {response}\\n\")\n",
    "\n",
    "# # Run test\n",
    "# if __name__ == \"__main__\":\n",
    "#     test_chat_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e926b-4007-4ba5-8655-1d3b81975907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85e7dd5c-0a45-4492-99d8-744d00c07b71",
   "metadata": {},
   "source": [
    "#### 3 : Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045f6873-acac-459d-b9bd-522bf9c9f9c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Purpose:** Uses an **LLM-as-a-judge** to **score and compare** AI model responses.\n",
    "\n",
    "**Core idea:** Instead of humans rating answers, you use **one LLM** to evaluate **other LLMs' outputs**.\n",
    "\n",
    "What it does:\n",
    "1.  **`evaluate_response`**: Asks the LLM to **rate a single answer** (e.g., \"Accuracy: 4/5, Clarity: 3/5\").\n",
    "2.  **`benchmark_models`**: **Tests multiple models** on the same questions and compares their scores.\n",
    "\n",
    "Why it's useful for you:\n",
    "- **Test your agent**: Could evaluate if `Qwen` or `Llama` gives better `TOOL:` outputs.\n",
    "- **Improve prompts**: See which prompt makes the LLM follow `TOOL:` format more reliably.\n",
    "- **Quality check**: Automatically grade your agent's final answers.\n",
    "\n",
    "It's a **meta-evaluation tool** for improving your agent, not part of the agent itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd1a966-62fe-4454-97a4-7b4c80013e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d88f9fd4-0389-4064-8416-009cfa477408",
   "metadata": {},
   "source": [
    "Below is a **full, line-by-line, word-by-word explanation**, written **clearly and slowly**, with **what each part does and why it exists**.\n",
    "\n",
    "---\n",
    "\n",
    "`from typing import List, Dict`\n",
    "\n",
    "* Imports **type hint helpers** from Python.\n",
    "* `List` â†’ represents a list type (e.g. `List[str]`).\n",
    "* `Dict` â†’ represents a dictionary type (e.g. `Dict[str, int]`).\n",
    "* These **do not change behavior**; they improve readability and tooling.\n",
    "\n",
    "---\n",
    "\n",
    "`from datetime import datetime`\n",
    "\n",
    "* Imports the `datetime` class.\n",
    "* Used to **attach timestamps** to evaluations.\n",
    "\n",
    "---\n",
    "\n",
    "`# LLM Evaluator - Functional Version`\n",
    "\n",
    "* Comment.\n",
    "* Explains this file implements an **LLM evaluation system** using functions, not classes.\n",
    "\n",
    "---\n",
    "\n",
    "`def init_evaluator(llm_client):`\n",
    "\n",
    "* Defines a function named `init_evaluator`.\n",
    "* Purpose: **create and return evaluator state**.\n",
    "* `llm_client` â†’ function that calls an LLM.\n",
    "\n",
    "---\n",
    "\n",
    "`\"\"\"Initialize an evaluator (replaces __init__)\"\"\"`\n",
    "\n",
    "* Docstring.\n",
    "* Says this function replaces a class constructor (`__init__`).\n",
    "\n",
    "---\n",
    "\n",
    "`return {\"llm_client\": llm_client}`\n",
    "\n",
    "* Returns a **dictionary**.\n",
    "* Stores the LLM function so other functions can use it.\n",
    "\n",
    "---\n",
    "\n",
    "`def evaluate_response(evaluator, query: str, response: str, criteria: List[str]) -> Dict:`\n",
    "\n",
    "* Defines a function to **judge the quality of an AI response**.\n",
    "* Parameters:\n",
    "\n",
    "  * `evaluator` â†’ dictionary holding the LLM\n",
    "  * `query` â†’ userâ€™s question\n",
    "  * `response` â†’ AIâ€™s answer\n",
    "  * `criteria` â†’ list of evaluation rules\n",
    "* `-> Dict` means it returns a dictionary.\n",
    "\n",
    "---\n",
    "\n",
    "`\"\"\"Evaluate response quality\"\"\"`\n",
    "\n",
    "* Docstring.\n",
    "* Explains the functionâ€™s goal.\n",
    "\n",
    "---\n",
    "\n",
    "`criteria_text = \"\\n\".join([f\"- {c}\" for c in criteria])`\n",
    "\n",
    "* Builds a formatted list of criteria.\n",
    "* Example:\n",
    "\n",
    "  ```\n",
    "  - accuracy\n",
    "  - clarity\n",
    "  ```\n",
    "* This makes the prompt **clear for the evaluator LLM**.\n",
    "\n",
    "---\n",
    "\n",
    "`prompt = f\"\"\"Evaluate this AI response:`\n",
    "\n",
    "* Starts a **multi-line string** (triple quotes).\n",
    "* This is the evaluation instruction sent to the LLM.\n",
    "\n",
    "---\n",
    "\n",
    "Inside the prompt:\n",
    "\n",
    "```\n",
    "Query: {query}\n",
    "Response: {response}\n",
    "```\n",
    "\n",
    "* Shows the evaluator:\n",
    "\n",
    "  * What was asked\n",
    "  * What the AI answered\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "Evaluation Criteria:\n",
    "{criteria_text}\n",
    "```\n",
    "\n",
    "* Injects the formatted criteria list.\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "Rate each criterion 1-5 and provide brief comments:\n",
    "```\n",
    "\n",
    "* Tells the LLM **exactly what to do**.\n",
    "\n",
    "---\n",
    "\n",
    "`evaluation = evaluator[\"llm_client\"](prompt, max_tokens=200)`\n",
    "\n",
    "* Calls the LLM evaluator.\n",
    "* Sends:\n",
    "\n",
    "  * Evaluation prompt\n",
    "  * Token limit\n",
    "* The LLM becomes a **judge**, not a responder.\n",
    "\n",
    "---\n",
    "\n",
    "`return {`\n",
    "\n",
    "* Returns structured evaluation data.\n",
    "\n",
    "---\n",
    "\n",
    "`\"query\": query`\n",
    "\n",
    "* Stores the original question.\n",
    "\n",
    "---\n",
    "\n",
    "`\"response\": response`\n",
    "\n",
    "* Stores the AIâ€™s answer being judged.\n",
    "\n",
    "---\n",
    "\n",
    "`\"evaluation\": evaluation[\"generated_text\"]`\n",
    "\n",
    "* Extracts the evaluator LLMâ€™s judgment.\n",
    "\n",
    "---\n",
    "\n",
    "`\"timestamp\": datetime.utcnow().isoformat()`\n",
    "\n",
    "* Adds current UTC time.\n",
    "* `.isoformat()` â†’ standard machine-readable time string.\n",
    "\n",
    "---\n",
    "\n",
    "`}`\n",
    "\n",
    "* Ends return dictionary.\n",
    "\n",
    "---\n",
    "\n",
    "`def benchmark_models(evaluator, test_cases: List[Dict], models: List[str]) -> Dict:`\n",
    "\n",
    "* Defines a **benchmarking function**.\n",
    "* Purpose: test multiple models on multiple prompts.\n",
    "* Inputs:\n",
    "\n",
    "  * `evaluator` â†’ evaluation system\n",
    "  * `test_cases` â†’ prompts + criteria\n",
    "  * `models` â†’ model names\n",
    "* Returns all results.\n",
    "\n",
    "---\n",
    "\n",
    "`\"\"\"Benchmark multiple models on test cases\"\"\"`\n",
    "\n",
    "* Explains purpose.\n",
    "\n",
    "---\n",
    "\n",
    "`results = {}`\n",
    "\n",
    "* Empty dictionary to store benchmark results.\n",
    "\n",
    "---\n",
    "\n",
    "`for model_name in models:`\n",
    "\n",
    "* Loops through each model name.\n",
    "\n",
    "---\n",
    "\n",
    "`print(f\"Testing {model_name}...\")`\n",
    "\n",
    "* Displays progress.\n",
    "\n",
    "---\n",
    "\n",
    "`model_results = []`\n",
    "\n",
    "* Stores evaluations for one model.\n",
    "\n",
    "---\n",
    "\n",
    "`for test_case in test_cases:`\n",
    "\n",
    "* Loops through every test prompt.\n",
    "\n",
    "---\n",
    "\n",
    "`response = evaluator[\"llm_client\"]( test_case[\"prompt\"], max_tokens=test_case.get(\"max_tokens\", 100) )`\n",
    "\n",
    "* Calls the LLM to generate an answer.\n",
    "* Uses:\n",
    "\n",
    "  * Prompt\n",
    "  * Optional token limit (default = 100).\n",
    "\n",
    "---\n",
    "\n",
    "`evaluation = evaluate_response(...)`\n",
    "\n",
    "* Sends the generated answer to the evaluator.\n",
    "* Uses:\n",
    "\n",
    "  * Prompt\n",
    "  * AI response\n",
    "  * Criteria (defaulted if missing).\n",
    "\n",
    "---\n",
    "\n",
    "`model_results.append(evaluation)`\n",
    "\n",
    "* Saves this evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "`results[model_name] = model_results`\n",
    "\n",
    "* Stores all evaluations for that model.\n",
    "\n",
    "---\n",
    "\n",
    "`return results`\n",
    "\n",
    "* Returns full benchmark data.\n",
    "\n",
    "---\n",
    "\n",
    "`def mock_llm_client(prompt, **kwargs):`\n",
    "\n",
    "* Fake LLM function.\n",
    "* Used for testing without real API calls.\n",
    "\n",
    "---\n",
    "\n",
    "`\"\"\"Simple mock LLM for testing\"\"\"`\n",
    "\n",
    "* Explains itâ€™s a mock.\n",
    "\n",
    "---\n",
    "\n",
    "`return {\"generated_text\": f\"Mock evaluation for: {prompt[:30]}...\"}`\n",
    "\n",
    "* Returns fake output.\n",
    "* Truncates prompt to simulate a response.\n",
    "\n",
    "---\n",
    "\n",
    "`def test_evaluator():`\n",
    "\n",
    "* Defines a test function.\n",
    "\n",
    "---\n",
    "\n",
    "`\"\"\"Test the functional evaluator system\"\"\"`\n",
    "\n",
    "* Explains purpose.\n",
    "\n",
    "---\n",
    "\n",
    "`evaluator = init_evaluator(mock_llm_client)`\n",
    "\n",
    "* Creates evaluator using fake LLM.\n",
    "\n",
    "---\n",
    "\n",
    "`test_cases = [...]`\n",
    "\n",
    "* Defines test prompts.\n",
    "* Includes:\n",
    "\n",
    "  * Prompt\n",
    "  * Criteria\n",
    "  * Token limit\n",
    "\n",
    "---\n",
    "\n",
    "`evaluation = evaluate_response(...)`\n",
    "\n",
    "* Runs a single evaluation test.\n",
    "\n",
    "---\n",
    "\n",
    "`print(\"=== Evaluation Test ===\")`\n",
    "\n",
    "* Prints test header.\n",
    "\n",
    "---\n",
    "\n",
    "`print(f\"Evaluation: {evaluation['evaluation']}\")`\n",
    "\n",
    "* Displays evaluation result.\n",
    "\n",
    "---\n",
    "\n",
    "`models_to_test = [\"model_a\", \"model_b\"]`\n",
    "\n",
    "* Fake model names for benchmarking.\n",
    "\n",
    "---\n",
    "\n",
    "`benchmark_results = benchmark_models(...)`\n",
    "\n",
    "* Runs benchmark across models.\n",
    "\n",
    "---\n",
    "\n",
    "`for model, results in benchmark_results.items():`\n",
    "\n",
    "* Iterates through benchmark results.\n",
    "\n",
    "---\n",
    "\n",
    "`print(f\"\\n{model}: {len(results)} evaluations\")`\n",
    "\n",
    "* Prints number of evaluations per model.\n",
    "\n",
    "---\n",
    "\n",
    "`if __name__ == \"__main__\":`\n",
    "\n",
    "* Ensures test runs **only when file is executed directly**.\n",
    "\n",
    "---\n",
    "\n",
    "`test_evaluator()`\n",
    "\n",
    "* Runs the test suite.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **Big Picture**\n",
    "\n",
    "This code builds:\n",
    "\n",
    "* An **LLM-as-judge system**\n",
    "* Model benchmarking\n",
    "* Evaluation pipelines\n",
    "* Production-style testing\n",
    "* A foundation for **automatic model comparison**\n",
    "\n",
    "If you want next:\n",
    "\n",
    "* Turn this into an **agent evaluator**\n",
    "* Add **JSON scoring**\n",
    "* Plug into **real LLaMA / OpenAI**\n",
    "* Build **leaderboards**\n",
    "\n",
    "Say which direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "033ecbd0-6db9-49b3-a505-ca399921e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "830e7ade-c53a-4d60-a8d0-db54339d7c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_evaluator(llm_client):\n",
    "    return {'llm_client': llm_client}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91b8bdd4-3a70-4d18-b8ee-1f20eb4bbd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator â†’ dictionary holding the LLM | query â†’ userâ€™s question | response â†’ AIâ€™s answer | criteria â†’ list of evaluation rules\n",
    "def evalute_response(evaluator, query, response, criteria):\n",
    "    criteria_text = '\\n'.join([f'- {c}' for c in criteria])\n",
    "    \n",
    "    prompt = f\"\"\"Evaluate this AI response:\n",
    "Query: {query}\n",
    "Response: {response}\n",
    "Evaluation Criteria: {criteria_text}\n",
    "Rate each criterion 1-5 and provide brief comments:\"\"\"\n",
    "    evaluation = evaluator['llm_client'](prompt, max_tokens=50)\n",
    "\n",
    "    return {\n",
    "        'query': query,\n",
    "        'response': response,\n",
    "        'evaluation': evaluation['generated_text'],\n",
    "        'timestamp': datetime.utcnow().isoformat()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f863b611-57dc-488a-af59-ce67f0770313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_real_models(test_cases: List[Dict], model_clients: Dict[str, callable]) -> Dict:\n",
    "    \"\"\"Benchmark multiple REAL models\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for model_name, llm_client in model_clients.items():\n",
    "        print(f\"Testing {model_name}...\")\n",
    "        model_results = []\n",
    "        \n",
    "        # Create evaluator WITH THIS SPECIFIC CLIENT\n",
    "        evaluator = init_evaluator(llm_client)\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            # Get response from THIS model\n",
    "            response = llm_client(\n",
    "                test_case[\"prompt\"], \n",
    "                max_tokens=test_case.get(\"max_tokens\", 50)\n",
    "            )\n",
    "            \n",
    "            # Evaluate with THIS model as judge (or use a neutral judge)\n",
    "            evaluation = evalute_response(\n",
    "                evaluator,  # This model judges itself\n",
    "                test_case[\"prompt\"],\n",
    "                response[\"generated_text\"],\n",
    "                test_case.get(\"criteria\", [\"relevance\", \"accuracy\", \"coherence\"])\n",
    "            )\n",
    "            \n",
    "            model_results.append(evaluation)\n",
    "        \n",
    "        results[model_name] = model_results\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "044d6258-be67-4cb3-a2da-8c86b587d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llama(prompt, max_tokens=50):\n",
    "    # API URL (new router system)\n",
    "    API_URL = \"https://router.huggingface.co/v1/chat/completions\"\n",
    "    \n",
    "    # Headers with your REAL token\n",
    "    headers = {\"Authorization\": \"Bearer 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\"}  # Replace with your token\n",
    "    \n",
    "    # Payload with model and parameters\n",
    "    payload = {\n",
    "        \"model\": \"Qwen/Qwen2.5-Coder-7B-Instruct\",  # Also available\n",
    "        # \"model\": \"meta-llama/Llama-3.1-8b-instruct\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.1  # Makes output more consistent\n",
    "    }\n",
    "    \n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    response_data = response.json()\n",
    "    \n",
    "    try:\n",
    "        return {\"generated_text\": response_data['choices'][0]['message']['content']}\n",
    "    except KeyError:\n",
    "        return {\"generated_text\": f\"API Error: {response_data}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f144c82e-1e80-4b05-9066-8cde6c30d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = init_evaluator(query_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a25600b0-cef7-46c0-a740-e7fc3c6e3c10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HELLO!\\AppData\\Local\\Temp\\ipykernel_17988\\4192351008.py:16: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  'timestamp': datetime.utcnow().isoformat()}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate a response\n",
    "result = evalute_response(\n",
    "    evaluator,\n",
    "    \"What's 2+2?\",\n",
    "    \"The answer is 4\",\n",
    "    [\"accuracy\", \"clarity\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f4526a1-ec21-4ea3-96b9-2db485bf5c15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: What's 2+2?\n",
      "response: The answer is 4\n",
      "evaluation: Accuracy: 5/5\n",
      "The response correctly calculates the sum of 2+2 as 4.\n",
      "\n",
      "Clarity: 5/5\n",
      "The response is straightforward and easy to understand, providing a clear and concise answer to the query.\n",
      "timestamp: 2025-12-27T22:22:55.933276\n"
     ]
    }
   ],
   "source": [
    "for i, j in result.items():\n",
    "        print(f'{i}: {j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e3ca833-ff53-47c6-8ef6-d85ee20b1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    {\n",
    "        \"prompt\": \"Explain quantum computing basics\",\n",
    "        \"criteria\": [\"accuracy\", \"clarity\", \"completeness\"],\n",
    "        \"max_tokens\": 50}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bb4fe15-c5f9-4a62-a347-26e8f03ee23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Qwen...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HELLO!\\AppData\\Local\\Temp\\ipykernel_11340\\4192351008.py:16: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  'timestamp': datetime.utcnow().isoformat()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Llama...\n"
     ]
    }
   ],
   "source": [
    "# Define your actual API clients\n",
    "model_clients = {\n",
    "    \"Qwen\": query_llama,\n",
    "    \"Llama\": query_llama  # Same function\n",
    "}\n",
    "\n",
    "# Run benchmark\n",
    "benchmark = benchmark_real_models(test_cases, model_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f8219e3-1520-4523-ad92-3c58bd79565a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'Explain quantum computing basics',\n",
       "  'response': 'Quantum computing is a type of computing that uses the principles of quantum mechanics to perform operations on data. Unlike classical computers, which use bits as the basic unit of information (with each bit being either 0 or 1), quantum computers use quantum',\n",
       "  'evaluation': '**Accuracy:** 4/5  \\nThe response accurately describes the fundamental concept of quantum computing, distinguishing it from classical computing in terms of its use of quantum bits (qubits) instead of classical bits.\\n\\n**Clarity:** 3/5  \\n',\n",
       "  'timestamp': '2025-12-27T22:10:23.892007'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark['Qwen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3ed8fc7-b6ac-4e18-a720-af575d5737b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'Explain quantum computing basics',\n",
       "  'response': 'Quantum computing is a type of computing that uses quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data. Unlike classical computers, which use bits represented by either 0 or 1, quantum computers use',\n",
       "  'evaluation': \"**Accuracy:** 4/5  \\nThe response accurately describes the fundamental concepts of quantum computing, including the use of quantum-mechanical phenomena like superposition and entanglement, and contrasts it with classical computing's use of bits.\\n\\n**Clarity\",\n",
       "  'timestamp': '2025-12-27T22:10:36.376420'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark['Llama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea0c9b-5609-4b4f-8f3b-9a5e361bf73d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e097a3-8d3f-4551-ae1c-aead9a2fbd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c588ca77",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from typing import List, Dict\n",
    "# from datetime import datetime\n",
    "\n",
    "# # LLM Evaluator - Functional Version\n",
    "# def init_evaluator(llm_client):\n",
    "#     \"\"\"Initialize an evaluator (replaces __init__)\"\"\"\n",
    "#     return {\"llm_client\": llm_client}\n",
    "\n",
    "# def evaluate_response(evaluator, query: str, response: str, criteria: List[str]) -> Dict:\n",
    "#     \"\"\"Evaluate response quality\"\"\"\n",
    "    \n",
    "#     criteria_text = \"\\n\".join([f\"- {c}\" for c in criteria])\n",
    "    \n",
    "#     prompt = f\"\"\"Evaluate this AI response:\n",
    "\n",
    "# Query: {query}\n",
    "# Response: {response}\n",
    "\n",
    "# Evaluation Criteria:\n",
    "# {criteria_text}\n",
    "\n",
    "# Rate each criterion 1-5 and provide brief comments:\"\"\"\n",
    "    \n",
    "#     # Note: Changed .generate() to direct call to match your llm_client\n",
    "#     evaluation = evaluator[\"llm_client\"](prompt, max_tokens=200)\n",
    "    \n",
    "#     return {\n",
    "#         \"query\": query,\n",
    "#         \"response\": response, \n",
    "#         \"evaluation\": evaluation[\"generated_text\"],\n",
    "#         \"timestamp\": datetime.utcnow().isoformat()\n",
    "#     }\n",
    "\n",
    "# def benchmark_models(evaluator, test_cases: List[Dict], models: List[str]) -> Dict:\n",
    "#     \"\"\"Benchmark multiple models on test cases\"\"\"\n",
    "#     results = {}\n",
    "    \n",
    "#     for model_name in models:\n",
    "#         print(f\"Testing {model_name}...\")\n",
    "#         model_results = []\n",
    "        \n",
    "#         for test_case in test_cases:\n",
    "#             # In real implementation, switch models here\n",
    "#             # For now, using the same llm_client\n",
    "#             response = evaluator[\"llm_client\"](\n",
    "#                 test_case[\"prompt\"], \n",
    "#                 max_tokens=test_case.get(\"max_tokens\", 100)\n",
    "#             )\n",
    "            \n",
    "#             evaluation = evaluate_response(\n",
    "#                 evaluator,\n",
    "#                 test_case[\"prompt\"],\n",
    "#                 response[\"generated_text\"],\n",
    "#                 test_case.get(\"criteria\", [\"relevance\", \"accuracy\", \"coherence\"])\n",
    "#             )\n",
    "            \n",
    "#             model_results.append(evaluation)\n",
    "        \n",
    "#         results[model_name] = model_results\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # Mock LLM client for testing\n",
    "# def mock_llm_client(prompt, **kwargs):\n",
    "#     \"\"\"Simple mock LLM for testing\"\"\"\n",
    "#     return {\"generated_text\": f\"Mock evaluation for: {prompt[:30]}...\"}\n",
    "\n",
    "# # Test the functional evaluator\n",
    "# def test_evaluator():\n",
    "#     \"\"\"Test the functional evaluator system\"\"\"\n",
    "#     # Initialize evaluator\n",
    "#     evaluator = init_evaluator(mock_llm_client)\n",
    "    \n",
    "#     test_cases = [\n",
    "#         {\n",
    "#             \"prompt\": \"Explain quantum computing basics\",\n",
    "#             \"criteria\": [\"accuracy\", \"clarity\", \"completeness\"],\n",
    "#             \"max_tokens\": 100\n",
    "#         }\n",
    "#     ]\n",
    "    \n",
    "#     evaluation = evaluate_response(\n",
    "#         evaluator,\n",
    "#         \"Explain quantum computing basics\",\n",
    "#         \"Quantum computing uses qubits that can be in multiple states at once.\",\n",
    "#         [\"accuracy\", \"clarity\"]\n",
    "#     )\n",
    "    \n",
    "#     print(\"=== Evaluation Test ===\")\n",
    "#     print(f\"Evaluation: {evaluation['evaluation']}\")\n",
    "    \n",
    "#     # Test benchmark (with dummy model names)\n",
    "#     print(\"\\n=== Benchmark Test ===\")\n",
    "#     models_to_test = [\"model_a\", \"model_b\"]\n",
    "#     benchmark_results = benchmark_models(evaluator, test_cases, models_to_test)\n",
    "    \n",
    "#     for model, results in benchmark_results.items():\n",
    "#         print(f\"\\n{model}: {len(results)} evaluations\")\n",
    "\n",
    "# # Run test\n",
    "# if __name__ == \"__main__\":\n",
    "#     test_evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5411f628",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Create evaluator with your actual LLM\n",
    "# evaluator = init_evaluator(query_llama)\n",
    "\n",
    "# # Evaluate a response\n",
    "# result = evaluate_response(\n",
    "#     evaluator,\n",
    "#     \"What's 2+2?\",\n",
    "#     \"The answer is 4\",\n",
    "#     [\"accuracy\", \"clarity\"]\n",
    "# )\n",
    "\n",
    "# # Benchmark models\n",
    "# benchmark = benchmark_models(\n",
    "#     evaluator,\n",
    "#     test_cases, \n",
    "#     [\"Qwen-7B\", \"Llama-8B\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f39b92f1-09fc-488e-970d-96dd17207ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
